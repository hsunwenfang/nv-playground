
==> Audit <==
|----------------|--------------------------------|----------|-----------|---------|---------------------|---------------------|
|    Command     |              Args              | Profile  |   User    | Version |     Start Time      |      End Time       |
|----------------|--------------------------------|----------|-----------|---------|---------------------|---------------------|
| start          | --driver=docker                | minikube | azureuser | v1.36.0 | 05 Sep 25 01:58 UTC |                     |
|                | --container-runtime=containerd |          |           |         |                     |                     |
|                | --memory=8192 --cpus=4         |          |           |         |                     |                     |
| delete         | --profile=nv-local             | nv-local | azureuser | v1.36.0 | 05 Sep 25 02:38 UTC | 05 Sep 25 02:38 UTC |
| start          | --profile=nv-local             | nv-local | azureuser | v1.36.0 | 05 Sep 25 02:38 UTC | 05 Sep 25 02:40 UTC |
|                | --driver=docker                |          |           |         |                     |                     |
|                | --container-runtime=containerd |          |           |         |                     |                     |
|                | --memory=4096 --cpus=2         |          |           |         |                     |                     |
| image          | load nv-mnist:latest           | minikube | azureuser | v1.36.0 | 05 Sep 25 02:42 UTC |                     |
| delete         | --profile=nv-local             | nv-local | azureuser | v1.36.0 | 05 Sep 25 02:46 UTC | 05 Sep 25 02:46 UTC |
| start          | --profile=nv-local             | nv-local | azureuser | v1.36.0 | 05 Sep 25 02:46 UTC | 05 Sep 25 02:47 UTC |
|                | --driver=docker                |          |           |         |                     |                     |
|                | --container-runtime=containerd |          |           |         |                     |                     |
|                | --memory=4096 --cpus=2         |          |           |         |                     |                     |
| image          | load -p nv-local               | nv-local | azureuser | v1.36.0 | 05 Sep 25 02:47 UTC | 05 Sep 25 02:58 UTC |
|                | nv-mnist:latest                |          |           |         |                     |                     |
| start          | --container-runtime=containerd | minikube | azureuser | v1.36.0 | 05 Sep 25 03:16 UTC | 05 Sep 25 03:17 UTC |
|                | --driver=docker                |          |           |         |                     |                     |
| start          | --container-runtime=containerd | minikube | azureuser | v1.36.0 | 05 Sep 25 03:18 UTC | 05 Sep 25 03:18 UTC |
|                | --driver=docker                |          |           |         |                     |                     |
| update-context |                                | minikube | azureuser | v1.36.0 | 05 Sep 25 03:18 UTC | 05 Sep 25 03:18 UTC |
| start          | --container-runtime=containerd | minikube | azureuser | v1.36.0 | 05 Sep 25 03:18 UTC | 05 Sep 25 03:18 UTC |
|                | --driver=docker                |          |           |         |                     |                     |
| update-context |                                | minikube | azureuser | v1.36.0 | 05 Sep 25 03:19 UTC | 05 Sep 25 03:19 UTC |
| start          | --container-runtime=containerd | minikube | azureuser | v1.36.0 | 05 Sep 25 03:19 UTC | 05 Sep 25 03:20 UTC |
|                | --driver=docker                |          |           |         |                     |                     |
| start          | --container-runtime=containerd | minikube | azureuser | v1.36.0 | 05 Sep 25 03:21 UTC | 05 Sep 25 03:21 UTC |
|                | --driver=docker                |          |           |         |                     |                     |
| start          | --container-runtime=containerd | minikube | azureuser | v1.36.0 | 05 Sep 25 03:21 UTC | 05 Sep 25 03:21 UTC |
|                | --driver=docker                |          |           |         |                     |                     |
| start          | --container-runtime=containerd | minikube | azureuser | v1.36.0 | 05 Sep 25 03:22 UTC | 05 Sep 25 03:22 UTC |
|                | --driver=docker                |          |           |         |                     |                     |
| docker-env     | minikube docker-env            | minikube | azureuser | v1.36.0 | 05 Sep 25 03:23 UTC | 05 Sep 25 03:23 UTC |
| docker-env     |                                | minikube | azureuser | v1.36.0 | 05 Sep 25 03:27 UTC | 05 Sep 25 03:27 UTC |
| update-context |                                | minikube | azureuser | v1.36.0 | 05 Sep 25 03:27 UTC | 05 Sep 25 03:27 UTC |
| update-context |                                | minikube | azureuser | v1.36.0 | 05 Sep 25 03:27 UTC | 05 Sep 25 03:27 UTC |
| stop           |                                | minikube | azureuser | v1.36.0 | 05 Sep 25 03:27 UTC | 05 Sep 25 03:27 UTC |
| start          | --container-runtime=containerd | minikube | azureuser | v1.36.0 | 05 Sep 25 03:27 UTC | 05 Sep 25 03:28 UTC |
|                | --driver=docker                |          |           |         |                     |                     |
| stop           |                                | minikube | azureuser | v1.36.0 | 05 Sep 25 03:28 UTC | 05 Sep 25 03:28 UTC |
| start          | --container-runtime=containerd | minikube | azureuser | v1.36.0 | 05 Sep 25 03:28 UTC | 05 Sep 25 03:28 UTC |
|                | --driver=docker                |          |           |         |                     |                     |
| stop           |                                | minikube | azureuser | v1.36.0 | 05 Sep 25 03:28 UTC | 05 Sep 25 03:29 UTC |
| start          | --container-runtime=containerd | minikube | azureuser | v1.36.0 | 05 Sep 25 03:29 UTC | 05 Sep 25 03:29 UTC |
|                | --driver=docker                |          |           |         |                     |                     |
| docker-env     | minikube docker-env            | minikube | azureuser | v1.36.0 | 05 Sep 25 03:30 UTC | 05 Sep 25 03:30 UTC |
| docker-env     | minikube docker-env            | minikube | azureuser | v1.36.0 | 05 Sep 25 03:32 UTC | 05 Sep 25 03:32 UTC |
| docker-env     | minikube docker-env            | minikube | azureuser | v1.36.0 | 05 Sep 25 03:33 UTC | 05 Sep 25 03:33 UTC |
| docker-env     | minikube docker-env            | minikube | azureuser | v1.36.0 | 05 Sep 25 03:33 UTC | 05 Sep 25 03:33 UTC |
| docker-env     | minikube docker-env            | minikube | azureuser | v1.36.0 | 05 Sep 25 03:33 UTC | 05 Sep 25 03:33 UTC |
| docker-env     | minikube docker-env            | minikube | azureuser | v1.36.0 | 05 Sep 25 03:33 UTC | 05 Sep 25 03:33 UTC |
| docker-env     | minikube docker-env            | minikube | azureuser | v1.36.0 | 05 Sep 25 03:34 UTC | 05 Sep 25 03:34 UTC |
| docker-env     | minikube docker-env            | minikube | azureuser | v1.36.0 | 05 Sep 25 03:34 UTC | 05 Sep 25 03:34 UTC |
| docker-env     | minikube docker-env            | minikube | azureuser | v1.36.0 | 05 Sep 25 03:34 UTC | 05 Sep 25 03:34 UTC |
| docker-env     | minikube docker-env            | minikube | azureuser | v1.36.0 | 05 Sep 25 03:34 UTC | 05 Sep 25 03:34 UTC |
| docker-env     | minikube docker-env            | minikube | azureuser | v1.36.0 | 05 Sep 25 03:34 UTC | 05 Sep 25 03:34 UTC |
| docker-env     | minikube docker-env            | minikube | azureuser | v1.36.0 | 05 Sep 25 03:38 UTC | 05 Sep 25 03:38 UTC |
| docker-env     | minikube docker-env            | minikube | azureuser | v1.36.0 | 05 Sep 25 03:39 UTC | 05 Sep 25 03:39 UTC |
| stop           |                                | minikube | azureuser | v1.36.0 | 05 Sep 25 03:50 UTC | 05 Sep 25 03:50 UTC |
| start          | --driver=docker --gpus=all     | minikube | azureuser | v1.36.0 | 05 Sep 25 03:52 UTC | 05 Sep 25 03:52 UTC |
| stop           |                                | minikube | azureuser | v1.36.0 | 05 Sep 25 03:53 UTC |                     |
| stop           |                                | minikube | azureuser | v1.36.0 | 05 Sep 25 03:55 UTC | 05 Sep 25 03:55 UTC |
| delete         |                                | minikube | azureuser | v1.36.0 | 05 Sep 25 03:55 UTC | 05 Sep 25 03:55 UTC |
| start          | --driver=docker --gpus=all     | minikube | azureuser | v1.36.0 | 05 Sep 25 03:55 UTC | 05 Sep 25 03:56 UTC |
| image          | load nv-mnist:latest           | minikube | azureuser | v1.36.0 | 05 Sep 25 04:00 UTC | 05 Sep 25 04:16 UTC |
| image          | list                           | minikube | azureuser | v1.36.0 | 05 Sep 25 05:22 UTC | 05 Sep 25 05:22 UTC |
| image          | load nv-mnist:latest           | minikube | azureuser | v1.36.0 | 05 Sep 25 05:53 UTC | 05 Sep 25 06:05 UTC |
| image          | load nv-mnist:latest           | minikube | azureuser | v1.36.0 | 05 Sep 25 06:14 UTC |                     |
| image          | list                           | minikube | azureuser | v1.36.0 | 05 Sep 25 06:51 UTC | 05 Sep 25 06:51 UTC |
| image          | load nv-mnist:latest --v=7     | minikube | azureuser | v1.36.0 | 05 Sep 25 06:56 UTC |                     |
| image          | load nv-mnist:latest --v=7     | minikube | azureuser | v1.36.0 | 05 Sep 25 06:57 UTC |                     |
| image          | load nv-mnist:latest --v=7     | minikube | azureuser | v1.36.0 | 05 Sep 25 07:00 UTC |                     |
|----------------|--------------------------------|----------|-----------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/09/05 03:55:47
Running on machine: 5ea965d983e54228905424e95a5201ab000000
Binary: Built with gc go1.24.0 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0905 03:55:47.094143   87328 out.go:345] Setting OutFile to fd 1 ...
I0905 03:55:47.094342   87328 out.go:397] isatty.IsTerminal(1) = true
I0905 03:55:47.094349   87328 out.go:358] Setting ErrFile to fd 2...
I0905 03:55:47.094357   87328 out.go:397] isatty.IsTerminal(2) = true
I0905 03:55:47.094655   87328 root.go:338] Updating PATH: /home/azureuser/.minikube/bin
I0905 03:55:47.095554   87328 out.go:352] Setting JSON to false
I0905 03:55:47.097376   87328 start.go:130] hostinfo: {"hostname":"5ea965d983e54228905424e95a5201ab000000","uptime":27837,"bootTime":1757016710,"procs":305,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.8.0-1026-azure","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"47498d77-c190-ca42-b731-3f8d75fd3d1c"}
I0905 03:55:47.097451   87328 start.go:140] virtualization:  
I0905 03:55:47.106766   87328 out.go:177] 😄  minikube v1.36.0 on Ubuntu 22.04
I0905 03:55:47.113278   87328 notify.go:220] Checking for updates...
I0905 03:55:47.113878   87328 config.go:182] Loaded profile config "nv-local": Driver=docker, ContainerRuntime=containerd, KubernetesVersion=v1.33.1
I0905 03:55:47.114026   87328 driver.go:404] Setting default libvirt URI to qemu:///system
I0905 03:55:47.157392   87328 docker.go:123] docker version: linux-24.0.9-1:
I0905 03:55:47.157511   87328 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0905 03:55:47.243153   87328 info.go:266] docker info: {ID:ba61d96f-0a9f-4019-9ac7-8fb861acc6ad Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff false] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:22 OomKillDisable:true NGoroutines:34 SystemTime:2025-09-05 03:55:47.217393454 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:6.8.0-1026-azure OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:118159077376 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:5ea965d983e54228905424e95a5201ab000000 Labels:[] ExperimentalBuild:false ServerVersion:24.0.9-1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:nvidia Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:05044ec0a9a75232cad458027ca83437aae3f4da} RuncCommit:{ID:bc20cb4497af9af01bea4a8044f1678ffca2745c Expected:bc20cb4497af9af01bea4a8044f1678ffca2745c} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:0.22.0-1]] Warnings:<nil>}}
I0905 03:55:47.243279   87328 docker.go:318] overlay module found
I0905 03:55:47.250006   87328 out.go:177] ✨  Using the docker driver based on user configuration
I0905 03:55:47.255328   87328 start.go:304] selected driver: docker
I0905 03:55:47.255337   87328 start.go:908] validating driver "docker" against <nil>
I0905 03:55:47.255351   87328 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0905 03:55:47.255492   87328 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0905 03:55:47.336565   87328 info.go:266] docker info: {ID:ba61d96f-0a9f-4019-9ac7-8fb861acc6ad Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff false] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:22 OomKillDisable:true NGoroutines:34 SystemTime:2025-09-05 03:55:47.315289701 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:6.8.0-1026-azure OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:118159077376 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:5ea965d983e54228905424e95a5201ab000000 Labels:[] ExperimentalBuild:false ServerVersion:24.0.9-1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:nvidia Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:05044ec0a9a75232cad458027ca83437aae3f4da} RuncCommit:{ID:bc20cb4497af9af01bea4a8044f1678ffca2745c Expected:bc20cb4497af9af01bea4a8044f1678ffca2745c} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:0.22.0-1]] Warnings:<nil>}}
I0905 03:55:47.336759   87328 start_flags.go:311] no existing cluster config was found, will generate one from the flags 
I0905 03:55:47.337346   87328 start_flags.go:394] Using suggested 28100MB memory alloc based on sys=112685MB, container=112685MB
I0905 03:55:47.337538   87328 start_flags.go:958] Wait components to verify : map[apiserver:true system_pods:true]
I0905 03:55:47.343437   87328 out.go:177] 📌  Using Docker driver with root privileges
I0905 03:55:47.348241   87328 cni.go:84] Creating CNI manager for ""
I0905 03:55:47.348288   87328 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0905 03:55:47.348296   87328 start_flags.go:320] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0905 03:55:47.348396   87328 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:28100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/azureuser:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs:all AutoPauseInterval:1m0s}
I0905 03:55:47.353407   87328 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0905 03:55:47.358126   87328 cache.go:121] Beginning downloading kic base image for docker with docker
I0905 03:55:47.367006   87328 out.go:177] 🚜  Pulling base image v0.0.47 ...
I0905 03:55:47.371841   87328 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0905 03:55:47.371953   87328 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0905 03:55:47.401150   87328 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon, skipping pull
I0905 03:55:47.401164   87328 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in daemon, skipping load
I0905 03:55:47.413099   87328 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.33.1/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0905 03:55:47.413114   87328 cache.go:56] Caching tarball of preloaded images
I0905 03:55:47.413296   87328 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0905 03:55:47.418640   87328 out.go:177] 💾  Downloading Kubernetes v1.33.1 preload ...
I0905 03:55:47.425216   87328 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 ...
I0905 03:55:47.503116   87328 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.33.1/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4?checksum=md5:426761ca53f0b79ef3d0619b4605274e -> /home/azureuser/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0905 03:55:51.081076   87328 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 ...
I0905 03:55:51.081228   87328 preload.go:254] verifying checksum of /home/azureuser/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 ...
I0905 03:55:52.188758   87328 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0905 03:55:52.188896   87328 profile.go:143] Saving config to /home/azureuser/.minikube/profiles/minikube/config.json ...
I0905 03:55:52.188935   87328 lock.go:35] WriteFile acquiring /home/azureuser/.minikube/profiles/minikube/config.json: {Name:mk6a0e0d8f3dc9800bfdf69e7e72dc5925a8b5e9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0905 03:55:52.189165   87328 cache.go:230] Successfully downloaded all kic artifacts
I0905 03:55:52.189189   87328 start.go:360] acquireMachinesLock for minikube: {Name:mkdcff04a72254afe96192e51aa6ae202882bdaf Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0905 03:55:52.189245   87328 start.go:364] duration metric: took 43.498µs to acquireMachinesLock for "minikube"
I0905 03:55:52.189261   87328 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:28100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/azureuser:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs:all AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0905 03:55:52.189327   87328 start.go:125] createHost starting for "" (driver="docker")
I0905 03:55:52.195776   87328 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=28100MB) ...
I0905 03:55:52.196080   87328 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0905 03:55:52.196105   87328 client.go:168] LocalClient.Create starting
I0905 03:55:52.196177   87328 main.go:141] libmachine: Reading certificate data from /home/azureuser/.minikube/certs/ca.pem
I0905 03:55:52.196222   87328 main.go:141] libmachine: Decoding PEM data...
I0905 03:55:52.196239   87328 main.go:141] libmachine: Parsing certificate...
I0905 03:55:52.196311   87328 main.go:141] libmachine: Reading certificate data from /home/azureuser/.minikube/certs/cert.pem
I0905 03:55:52.196417   87328 main.go:141] libmachine: Decoding PEM data...
I0905 03:55:52.196433   87328 main.go:141] libmachine: Parsing certificate...
I0905 03:55:52.196898   87328 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0905 03:55:52.220500   87328 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0905 03:55:52.220609   87328 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0905 03:55:52.220627   87328 cli_runner.go:164] Run: docker network inspect minikube
W0905 03:55:52.245092   87328 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0905 03:55:52.245116   87328 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0905 03:55:52.245136   87328 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0905 03:55:52.245249   87328 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0905 03:55:52.273542   87328 network.go:211] skipping subnet 192.168.49.0/24 that is taken: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName:br-b89b70256e20 IfaceIPv4:192.168.49.1 IfaceMTU:1500 IfaceMAC:02:42:5e:90:03:cf} reservation:<nil>}
I0905 03:55:52.274058   87328 network.go:206] using free private subnet 192.168.58.0/24: &{IP:192.168.58.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.58.0/24 Gateway:192.168.58.1 ClientMin:192.168.58.2 ClientMax:192.168.58.254 Broadcast:192.168.58.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc000407560}
I0905 03:55:52.274081   87328 network_create.go:124] attempt to create docker network minikube 192.168.58.0/24 with gateway 192.168.58.1 and MTU of 1500 ...
I0905 03:55:52.274151   87328 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.58.0/24 --gateway=192.168.58.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0905 03:55:52.413849   87328 network_create.go:108] docker network minikube 192.168.58.0/24 created
I0905 03:55:52.413874   87328 kic.go:121] calculated static IP "192.168.58.2" for the "minikube" container
I0905 03:55:52.413968   87328 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0905 03:55:52.437039   87328 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0905 03:55:52.463941   87328 oci.go:103] Successfully created a docker volume minikube
I0905 03:55:52.464053   87328 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib
I0905 03:55:56.756533   87328 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib: (4.292432359s)
I0905 03:55:56.756558   87328 oci.go:107] Successfully prepared a docker volume minikube
I0905 03:55:56.756579   87328 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0905 03:55:56.756603   87328 kic.go:194] Starting extracting preloaded images to volume ...
I0905 03:55:56.756692   87328 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/azureuser/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir
I0905 03:56:06.702923   87328 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/azureuser/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir: (9.946193712s)
I0905 03:56:06.702951   87328 kic.go:203] duration metric: took 9.946344405s to extract preloaded images to volume ...
W0905 03:56:06.703393   87328 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I0905 03:56:06.703514   87328 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0905 03:56:06.779135   87328 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.58.2 --gpus all --env NVIDIA_DRIVER_CAPABILITIES=all --volume minikube:/var --security-opt apparmor=unconfined --memory=28100mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b
I0905 03:56:08.386196   87328 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.58.2 --gpus all --env NVIDIA_DRIVER_CAPABILITIES=all --volume minikube:/var --security-opt apparmor=unconfined --memory=28100mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b: (1.606954725s)
I0905 03:56:08.386343   87328 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0905 03:56:08.417205   87328 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0905 03:56:08.448201   87328 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0905 03:56:08.539208   87328 oci.go:144] the created container "minikube" has a running status.
I0905 03:56:08.539303   87328 kic.go:225] Creating ssh key for kic: /home/azureuser/.minikube/machines/minikube/id_rsa...
I0905 03:56:09.734399   87328 kic_runner.go:191] docker (temp): /home/azureuser/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0905 03:56:09.798828   87328 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0905 03:56:09.822961   87328 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0905 03:56:09.822992   87328 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0905 03:56:09.914106   87328 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0905 03:56:09.944340   87328 machine.go:93] provisionDockerMachine start ...
I0905 03:56:09.944452   87328 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0905 03:56:09.974179   87328 main.go:141] libmachine: Using SSH client type: native
I0905 03:56:09.974551   87328 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0905 03:56:09.974566   87328 main.go:141] libmachine: About to run SSH command:
hostname
I0905 03:56:10.137054   87328 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0905 03:56:10.137077   87328 ubuntu.go:169] provisioning hostname "minikube"
I0905 03:56:10.137160   87328 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0905 03:56:10.163582   87328 main.go:141] libmachine: Using SSH client type: native
I0905 03:56:10.163900   87328 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0905 03:56:10.163912   87328 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0905 03:56:10.363537   87328 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0905 03:56:10.363632   87328 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0905 03:56:10.388825   87328 main.go:141] libmachine: Using SSH client type: native
I0905 03:56:10.389139   87328 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0905 03:56:10.389159   87328 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0905 03:56:10.540072   87328 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0905 03:56:10.540092   87328 ubuntu.go:175] set auth options {CertDir:/home/azureuser/.minikube CaCertPath:/home/azureuser/.minikube/certs/ca.pem CaPrivateKeyPath:/home/azureuser/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/azureuser/.minikube/machines/server.pem ServerKeyPath:/home/azureuser/.minikube/machines/server-key.pem ClientKeyPath:/home/azureuser/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/azureuser/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/azureuser/.minikube}
I0905 03:56:10.540134   87328 ubuntu.go:177] setting up certificates
I0905 03:56:10.540145   87328 provision.go:84] configureAuth start
I0905 03:56:10.540222   87328 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0905 03:56:10.563704   87328 provision.go:143] copyHostCerts
I0905 03:56:10.563764   87328 exec_runner.go:144] found /home/azureuser/.minikube/ca.pem, removing ...
I0905 03:56:10.563771   87328 exec_runner.go:203] rm: /home/azureuser/.minikube/ca.pem
I0905 03:56:10.563865   87328 exec_runner.go:151] cp: /home/azureuser/.minikube/certs/ca.pem --> /home/azureuser/.minikube/ca.pem (1086 bytes)
I0905 03:56:10.564031   87328 exec_runner.go:144] found /home/azureuser/.minikube/cert.pem, removing ...
I0905 03:56:10.564036   87328 exec_runner.go:203] rm: /home/azureuser/.minikube/cert.pem
I0905 03:56:10.564084   87328 exec_runner.go:151] cp: /home/azureuser/.minikube/certs/cert.pem --> /home/azureuser/.minikube/cert.pem (1131 bytes)
I0905 03:56:10.564188   87328 exec_runner.go:144] found /home/azureuser/.minikube/key.pem, removing ...
I0905 03:56:10.564192   87328 exec_runner.go:203] rm: /home/azureuser/.minikube/key.pem
I0905 03:56:10.564308   87328 exec_runner.go:151] cp: /home/azureuser/.minikube/certs/key.pem --> /home/azureuser/.minikube/key.pem (1675 bytes)
I0905 03:56:10.564410   87328 provision.go:117] generating server cert: /home/azureuser/.minikube/machines/server.pem ca-key=/home/azureuser/.minikube/certs/ca.pem private-key=/home/azureuser/.minikube/certs/ca-key.pem org=azureuser.minikube san=[127.0.0.1 192.168.58.2 localhost minikube]
I0905 03:56:11.096914   87328 provision.go:177] copyRemoteCerts
I0905 03:56:11.097013   87328 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0905 03:56:11.097073   87328 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0905 03:56:11.123495   87328 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/azureuser/.minikube/machines/minikube/id_rsa Username:docker}
I0905 03:56:11.231253   87328 ssh_runner.go:362] scp /home/azureuser/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1086 bytes)
I0905 03:56:11.276880   87328 ssh_runner.go:362] scp /home/azureuser/.minikube/machines/server.pem --> /etc/docker/server.pem (1188 bytes)
I0905 03:56:11.324427   87328 ssh_runner.go:362] scp /home/azureuser/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0905 03:56:11.374636   87328 provision.go:87] duration metric: took 834.475633ms to configureAuth
I0905 03:56:11.374659   87328 ubuntu.go:193] setting minikube options for container-runtime
I0905 03:56:11.374914   87328 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0905 03:56:11.375025   87328 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0905 03:56:11.403626   87328 main.go:141] libmachine: Using SSH client type: native
I0905 03:56:11.404052   87328 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0905 03:56:11.404063   87328 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0905 03:56:11.549214   87328 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0905 03:56:11.549226   87328 ubuntu.go:71] root file system type: overlay
I0905 03:56:11.549346   87328 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0905 03:56:11.549415   87328 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0905 03:56:11.573066   87328 main.go:141] libmachine: Using SSH client type: native
I0905 03:56:11.573332   87328 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0905 03:56:11.573417   87328 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0905 03:56:11.752415   87328 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0905 03:56:11.752520   87328 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0905 03:56:11.784874   87328 main.go:141] libmachine: Using SSH client type: native
I0905 03:56:11.785272   87328 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0905 03:56:11.785296   87328 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0905 03:56:14.032529   87328 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-04-18 09:50:48.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-09-05 03:56:11.749689409 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0905 03:56:14.032554   87328 machine.go:96] duration metric: took 4.088199134s to provisionDockerMachine
I0905 03:56:14.032569   87328 client.go:171] duration metric: took 21.836458041s to LocalClient.Create
I0905 03:56:14.032602   87328 start.go:167] duration metric: took 21.836522139s to libmachine.API.Create "minikube"
I0905 03:56:14.032611   87328 start.go:293] postStartSetup for "minikube" (driver="docker")
I0905 03:56:14.032625   87328 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0905 03:56:14.032709   87328 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0905 03:56:14.032828   87328 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0905 03:56:14.060196   87328 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/azureuser/.minikube/machines/minikube/id_rsa Username:docker}
I0905 03:56:14.178816   87328 ssh_runner.go:195] Run: cat /etc/os-release
I0905 03:56:14.183840   87328 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0905 03:56:14.183861   87328 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0905 03:56:14.183870   87328 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0905 03:56:14.183877   87328 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0905 03:56:14.183886   87328 filesync.go:126] Scanning /home/azureuser/.minikube/addons for local assets ...
I0905 03:56:14.183959   87328 filesync.go:126] Scanning /home/azureuser/.minikube/files for local assets ...
I0905 03:56:14.184026   87328 start.go:296] duration metric: took 151.406801ms for postStartSetup
I0905 03:56:14.184506   87328 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0905 03:56:14.214861   87328 profile.go:143] Saving config to /home/azureuser/.minikube/profiles/minikube/config.json ...
I0905 03:56:14.215274   87328 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0905 03:56:14.215330   87328 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0905 03:56:14.241574   87328 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/azureuser/.minikube/machines/minikube/id_rsa Username:docker}
I0905 03:56:14.350105   87328 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0905 03:56:14.357734   87328 start.go:128] duration metric: took 22.168393376s to createHost
I0905 03:56:14.357747   87328 start.go:83] releasing machines lock for "minikube", held for 22.168493471s
I0905 03:56:14.357829   87328 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0905 03:56:14.383693   87328 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0905 03:56:14.383779   87328 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0905 03:56:14.384503   87328 ssh_runner.go:195] Run: cat /version.json
I0905 03:56:14.384636   87328 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0905 03:56:14.410747   87328 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/azureuser/.minikube/machines/minikube/id_rsa Username:docker}
I0905 03:56:14.412871   87328 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/azureuser/.minikube/machines/minikube/id_rsa Username:docker}
I0905 03:56:14.664659   87328 ssh_runner.go:195] Run: systemctl --version
I0905 03:56:14.673094   87328 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0905 03:56:14.682173   87328 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0905 03:56:14.744060   87328 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0905 03:56:14.744133   87328 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0905 03:56:14.796241   87328 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0905 03:56:14.796256   87328 start.go:495] detecting cgroup driver to use...
I0905 03:56:14.796294   87328 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0905 03:56:14.796420   87328 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0905 03:56:14.829667   87328 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0905 03:56:14.853415   87328 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0905 03:56:14.872228   87328 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0905 03:56:14.872288   87328 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0905 03:56:14.890558   87328 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0905 03:56:14.908714   87328 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0905 03:56:14.927840   87328 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0905 03:56:14.946782   87328 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0905 03:56:14.967602   87328 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0905 03:56:14.988679   87328 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0905 03:56:15.012390   87328 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0905 03:56:15.032540   87328 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0905 03:56:15.050548   87328 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0905 03:56:15.070804   87328 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0905 03:56:15.243123   87328 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0905 03:56:15.381482   87328 start.go:495] detecting cgroup driver to use...
I0905 03:56:15.381530   87328 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0905 03:56:15.381604   87328 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0905 03:56:15.406573   87328 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0905 03:56:15.406663   87328 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0905 03:56:15.438364   87328 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0905 03:56:15.474947   87328 ssh_runner.go:195] Run: which cri-dockerd
I0905 03:56:15.481925   87328 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0905 03:56:15.597293   87328 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0905 03:56:15.649700   87328 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0905 03:56:15.832312   87328 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0905 03:56:16.009533   87328 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0905 03:56:16.009754   87328 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (243 bytes)
I0905 03:56:16.051076   87328 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0905 03:56:16.078156   87328 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0905 03:56:16.273073   87328 ssh_runner.go:195] Run: sudo systemctl restart docker
I0905 03:56:18.243371   87328 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.970268755s)
I0905 03:56:18.243445   87328 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0905 03:56:18.263286   87328 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0905 03:56:18.283014   87328 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0905 03:56:18.460317   87328 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0905 03:56:18.636613   87328 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0905 03:56:18.799217   87328 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0905 03:56:18.829579   87328 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0905 03:56:18.848379   87328 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0905 03:56:19.022547   87328 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0905 03:56:19.149921   87328 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0905 03:56:19.171081   87328 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0905 03:56:19.171160   87328 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0905 03:56:19.177232   87328 start.go:563] Will wait 60s for crictl version
I0905 03:56:19.177295   87328 ssh_runner.go:195] Run: which crictl
I0905 03:56:19.183032   87328 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0905 03:56:19.242223   87328 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0905 03:56:19.242301   87328 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0905 03:56:19.297109   87328 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0905 03:56:19.347291   87328 out.go:235] 🐳  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0905 03:56:19.347455   87328 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0905 03:56:19.373087   87328 ssh_runner.go:195] Run: grep 192.168.58.1	host.minikube.internal$ /etc/hosts
I0905 03:56:19.380068   87328 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.58.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0905 03:56:19.399717   87328 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:28100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/azureuser:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs:all AutoPauseInterval:1m0s} ...
I0905 03:56:19.399819   87328 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0905 03:56:19.399899   87328 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0905 03:56:19.427119   87328 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0905 03:56:19.427135   87328 docker.go:632] Images already preloaded, skipping extraction
I0905 03:56:19.427203   87328 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0905 03:56:19.462383   87328 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0905 03:56:19.462401   87328 cache_images.go:84] Images are preloaded, skipping loading
I0905 03:56:19.462411   87328 kubeadm.go:926] updating node { 192.168.58.2 8443 v1.33.1 docker true true} ...
I0905 03:56:19.462521   87328 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0905 03:56:19.462597   87328 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0905 03:56:19.548620   87328 cni.go:84] Creating CNI manager for ""
I0905 03:56:19.548640   87328 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0905 03:56:19.548653   87328 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0905 03:56:19.548678   87328 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0905 03:56:19.548847   87328 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.58.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0905 03:56:19.548931   87328 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0905 03:56:19.566450   87328 binaries.go:44] Found k8s binaries, skipping transfer
I0905 03:56:19.566520   87328 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0905 03:56:19.583224   87328 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0905 03:56:19.618667   87328 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0905 03:56:19.654200   87328 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0905 03:56:19.689643   87328 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0905 03:56:19.695298   87328 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0905 03:56:19.716346   87328 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0905 03:56:19.892738   87328 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0905 03:56:19.923763   87328 certs.go:68] Setting up /home/azureuser/.minikube/profiles/minikube for IP: 192.168.58.2
I0905 03:56:19.923773   87328 certs.go:194] generating shared ca certs ...
I0905 03:56:19.923792   87328 certs.go:226] acquiring lock for ca certs: {Name:mkabb2e586c5f4bf27122f0fb279a375a50390e0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0905 03:56:19.924019   87328 certs.go:235] skipping valid "minikubeCA" ca cert: /home/azureuser/.minikube/ca.key
I0905 03:56:19.924096   87328 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/azureuser/.minikube/proxy-client-ca.key
I0905 03:56:19.924107   87328 certs.go:256] generating profile certs ...
I0905 03:56:19.924186   87328 certs.go:363] generating signed profile cert for "minikube-user": /home/azureuser/.minikube/profiles/minikube/client.key
I0905 03:56:19.924211   87328 crypto.go:68] Generating cert /home/azureuser/.minikube/profiles/minikube/client.crt with IP's: []
I0905 03:56:20.590216   87328 crypto.go:156] Writing cert to /home/azureuser/.minikube/profiles/minikube/client.crt ...
I0905 03:56:20.590235   87328 lock.go:35] WriteFile acquiring /home/azureuser/.minikube/profiles/minikube/client.crt: {Name:mk90d29b4efaebe8aecdc19f1fb78527c924bee1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0905 03:56:20.590471   87328 crypto.go:164] Writing key to /home/azureuser/.minikube/profiles/minikube/client.key ...
I0905 03:56:20.590480   87328 lock.go:35] WriteFile acquiring /home/azureuser/.minikube/profiles/minikube/client.key: {Name:mk4126dbceb811504c699d5268affec6b3b17829 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0905 03:56:20.590624   87328 certs.go:363] generating signed profile cert for "minikube": /home/azureuser/.minikube/profiles/minikube/apiserver.key.502bbb95
I0905 03:56:20.590643   87328 crypto.go:68] Generating cert /home/azureuser/.minikube/profiles/minikube/apiserver.crt.502bbb95 with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.58.2]
I0905 03:56:22.110092   87328 crypto.go:156] Writing cert to /home/azureuser/.minikube/profiles/minikube/apiserver.crt.502bbb95 ...
I0905 03:56:22.110114   87328 lock.go:35] WriteFile acquiring /home/azureuser/.minikube/profiles/minikube/apiserver.crt.502bbb95: {Name:mkf4faf9f3032cf704be78b9c9391e383fd86a88 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0905 03:56:22.110325   87328 crypto.go:164] Writing key to /home/azureuser/.minikube/profiles/minikube/apiserver.key.502bbb95 ...
I0905 03:56:22.110336   87328 lock.go:35] WriteFile acquiring /home/azureuser/.minikube/profiles/minikube/apiserver.key.502bbb95: {Name:mk9f9b10c8b42e6480d88e60cc69a4f73f96b879 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0905 03:56:22.110463   87328 certs.go:381] copying /home/azureuser/.minikube/profiles/minikube/apiserver.crt.502bbb95 -> /home/azureuser/.minikube/profiles/minikube/apiserver.crt
I0905 03:56:22.110652   87328 certs.go:385] copying /home/azureuser/.minikube/profiles/minikube/apiserver.key.502bbb95 -> /home/azureuser/.minikube/profiles/minikube/apiserver.key
I0905 03:56:22.110784   87328 certs.go:363] generating signed profile cert for "aggregator": /home/azureuser/.minikube/profiles/minikube/proxy-client.key
I0905 03:56:22.110803   87328 crypto.go:68] Generating cert /home/azureuser/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0905 03:56:23.786659   87328 crypto.go:156] Writing cert to /home/azureuser/.minikube/profiles/minikube/proxy-client.crt ...
I0905 03:56:23.786680   87328 lock.go:35] WriteFile acquiring /home/azureuser/.minikube/profiles/minikube/proxy-client.crt: {Name:mk6f1d159d407fa88fd913ea0fa03ef4eb11bc94 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0905 03:56:23.786891   87328 crypto.go:164] Writing key to /home/azureuser/.minikube/profiles/minikube/proxy-client.key ...
I0905 03:56:23.786901   87328 lock.go:35] WriteFile acquiring /home/azureuser/.minikube/profiles/minikube/proxy-client.key: {Name:mka7dfc9102f5d57de0f34209136abc9109b5c4e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0905 03:56:23.787226   87328 certs.go:484] found cert: /home/azureuser/.minikube/certs/ca-key.pem (1675 bytes)
I0905 03:56:23.787276   87328 certs.go:484] found cert: /home/azureuser/.minikube/certs/ca.pem (1086 bytes)
I0905 03:56:23.787315   87328 certs.go:484] found cert: /home/azureuser/.minikube/certs/cert.pem (1131 bytes)
I0905 03:56:23.787350   87328 certs.go:484] found cert: /home/azureuser/.minikube/certs/key.pem (1675 bytes)
I0905 03:56:23.788123   87328 ssh_runner.go:362] scp /home/azureuser/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0905 03:56:23.832448   87328 ssh_runner.go:362] scp /home/azureuser/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0905 03:56:23.877966   87328 ssh_runner.go:362] scp /home/azureuser/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0905 03:56:23.925128   87328 ssh_runner.go:362] scp /home/azureuser/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0905 03:56:23.972689   87328 ssh_runner.go:362] scp /home/azureuser/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0905 03:56:24.016340   87328 ssh_runner.go:362] scp /home/azureuser/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0905 03:56:24.069469   87328 ssh_runner.go:362] scp /home/azureuser/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0905 03:56:24.118415   87328 ssh_runner.go:362] scp /home/azureuser/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0905 03:56:24.163692   87328 ssh_runner.go:362] scp /home/azureuser/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0905 03:56:24.216350   87328 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0905 03:56:24.250631   87328 ssh_runner.go:195] Run: openssl version
I0905 03:56:24.259268   87328 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0905 03:56:24.286278   87328 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0905 03:56:24.292043   87328 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep  5 02:39 /usr/share/ca-certificates/minikubeCA.pem
I0905 03:56:24.292097   87328 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0905 03:56:24.303071   87328 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0905 03:56:24.319883   87328 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0905 03:56:24.325152   87328 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0905 03:56:24.325207   87328 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:28100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/azureuser:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs:all AutoPauseInterval:1m0s}
I0905 03:56:24.325330   87328 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0905 03:56:24.352773   87328 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0905 03:56:24.369446   87328 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0905 03:56:24.386865   87328 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0905 03:56:24.387769   87328 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0905 03:56:24.406122   87328 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0905 03:56:24.406132   87328 kubeadm.go:157] found existing configuration files:

I0905 03:56:24.406195   87328 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0905 03:56:24.424088   87328 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0905 03:56:24.424153   87328 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0905 03:56:24.442357   87328 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0905 03:56:24.461092   87328 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0905 03:56:24.461165   87328 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0905 03:56:24.479834   87328 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0905 03:56:24.496721   87328 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0905 03:56:24.496779   87328 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0905 03:56:24.512219   87328 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0905 03:56:24.528090   87328 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0905 03:56:24.528147   87328 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0905 03:56:24.545325   87328 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0905 03:56:24.612899   87328 kubeadm.go:310] [init] Using Kubernetes version: v1.33.1
I0905 03:56:24.613330   87328 kubeadm.go:310] [preflight] Running pre-flight checks
I0905 03:56:24.656606   87328 kubeadm.go:310] [preflight] The system verification failed. Printing the output from the verification:
I0905 03:56:24.656726   87328 kubeadm.go:310] [0;37mKERNEL_VERSION[0m: [0;32m6.8.0-1026-azure[0m
I0905 03:56:24.656788   87328 kubeadm.go:310] [0;37mOS[0m: [0;32mLinux[0m
I0905 03:56:24.656870   87328 kubeadm.go:310] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I0905 03:56:24.656958   87328 kubeadm.go:310] [0;37mCGROUPS_CPUACCT[0m: [0;32menabled[0m
I0905 03:56:24.657064   87328 kubeadm.go:310] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I0905 03:56:24.657152   87328 kubeadm.go:310] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I0905 03:56:24.657300   87328 kubeadm.go:310] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I0905 03:56:24.657388   87328 kubeadm.go:310] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I0905 03:56:24.657460   87328 kubeadm.go:310] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I0905 03:56:24.657542   87328 kubeadm.go:310] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I0905 03:56:24.657647   87328 kubeadm.go:310] [0;37mCGROUPS_BLKIO[0m: [0;32menabled[0m
I0905 03:56:24.758576   87328 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0905 03:56:24.758760   87328 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0905 03:56:24.758934   87328 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0905 03:56:24.773186   87328 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0905 03:56:24.778874   87328 out.go:235]     ▪ Generating certificates and keys ...
I0905 03:56:24.779065   87328 kubeadm.go:310] [certs] Using existing ca certificate authority
I0905 03:56:24.779161   87328 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0905 03:56:25.186222   87328 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0905 03:56:25.800576   87328 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0905 03:56:26.086219   87328 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0905 03:56:26.166902   87328 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0905 03:56:26.577616   87328 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0905 03:56:26.577851   87328 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.58.2 127.0.0.1 ::1]
I0905 03:56:26.604799   87328 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0905 03:56:26.605110   87328 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.58.2 127.0.0.1 ::1]
I0905 03:56:26.733198   87328 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0905 03:56:26.934230   87328 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0905 03:56:27.305739   87328 kubeadm.go:310] [certs] Generating "sa" key and public key
I0905 03:56:27.305970   87328 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0905 03:56:27.495512   87328 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0905 03:56:27.672688   87328 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0905 03:56:28.624107   87328 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0905 03:56:29.002413   87328 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0905 03:56:29.259607   87328 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0905 03:56:29.260611   87328 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0905 03:56:29.270171   87328 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0905 03:56:29.279532   87328 out.go:235]     ▪ Booting up control plane ...
I0905 03:56:29.279746   87328 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0905 03:56:29.279942   87328 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0905 03:56:29.280079   87328 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0905 03:56:29.292076   87328 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0905 03:56:29.301812   87328 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0905 03:56:29.301889   87328 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0905 03:56:29.505001   87328 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0905 03:56:29.505199   87328 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0905 03:56:30.506644   87328 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.001779904s
I0905 03:56:30.510493   87328 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0905 03:56:30.510592   87328 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.58.2:8443/livez
I0905 03:56:30.510711   87328 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0905 03:56:30.510809   87328 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0905 03:56:35.630830   87328 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 5.120154701s
I0905 03:56:36.408613   87328 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 5.897815186s
I0905 03:56:38.512856   87328 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 8.002211512s
I0905 03:56:38.527346   87328 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0905 03:56:38.547662   87328 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0905 03:56:38.583907   87328 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0905 03:56:38.584296   87328 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0905 03:56:38.599385   87328 kubeadm.go:310] [bootstrap-token] Using token: 3ih6ie.9c92mcgv9cibb3we
I0905 03:56:38.604291   87328 out.go:235]     ▪ Configuring RBAC rules ...
I0905 03:56:38.604573   87328 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0905 03:56:38.611079   87328 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0905 03:56:38.626380   87328 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0905 03:56:38.632289   87328 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0905 03:56:38.637465   87328 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0905 03:56:38.642668   87328 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0905 03:56:38.921454   87328 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0905 03:56:39.440627   87328 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0905 03:56:39.926626   87328 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0905 03:56:39.927996   87328 kubeadm.go:310] 
I0905 03:56:39.928100   87328 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0905 03:56:39.928106   87328 kubeadm.go:310] 
I0905 03:56:39.928244   87328 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0905 03:56:39.928254   87328 kubeadm.go:310] 
I0905 03:56:39.928310   87328 kubeadm.go:310]   mkdir -p $HOME/.kube
I0905 03:56:39.928395   87328 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0905 03:56:39.928478   87328 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0905 03:56:39.928483   87328 kubeadm.go:310] 
I0905 03:56:39.928555   87328 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0905 03:56:39.928560   87328 kubeadm.go:310] 
I0905 03:56:39.928630   87328 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0905 03:56:39.928635   87328 kubeadm.go:310] 
I0905 03:56:39.928704   87328 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0905 03:56:39.928809   87328 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0905 03:56:39.928906   87328 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0905 03:56:39.928911   87328 kubeadm.go:310] 
I0905 03:56:39.929055   87328 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0905 03:56:39.929172   87328 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0905 03:56:39.929178   87328 kubeadm.go:310] 
I0905 03:56:39.929295   87328 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token 3ih6ie.9c92mcgv9cibb3we \
I0905 03:56:39.929461   87328 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:6bb3bbed887481836d89ac4bda868e7d11375e8d5f8782cb6d1f34f19accd0fa \
I0905 03:56:39.929492   87328 kubeadm.go:310] 	--control-plane 
I0905 03:56:39.929498   87328 kubeadm.go:310] 
I0905 03:56:39.929622   87328 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0905 03:56:39.929627   87328 kubeadm.go:310] 
I0905 03:56:39.929746   87328 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token 3ih6ie.9c92mcgv9cibb3we \
I0905 03:56:39.929902   87328 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:6bb3bbed887481836d89ac4bda868e7d11375e8d5f8782cb6d1f34f19accd0fa 
I0905 03:56:39.932950   87328 kubeadm.go:310] 	[WARNING SystemVerification]: cgroups v1 support is in maintenance mode, please migrate to cgroups v2
I0905 03:56:39.933334   87328 kubeadm.go:310] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/6.8.0-1026-azure\n", err: exit status 1
I0905 03:56:39.933511   87328 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0905 03:56:39.933558   87328 cni.go:84] Creating CNI manager for ""
I0905 03:56:39.933576   87328 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0905 03:56:39.938052   87328 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0905 03:56:39.943696   87328 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0905 03:56:39.960710   87328 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0905 03:56:39.993437   87328 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0905 03:56:39.993535   87328 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0905 03:56:39.993581   87328 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_09_05T03_56_39_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0905 03:56:40.010403   87328 ops.go:34] apiserver oom_adj: -16
I0905 03:56:40.176420   87328 kubeadm.go:1105] duration metric: took 182.955321ms to wait for elevateKubeSystemPrivileges
I0905 03:56:40.176780   87328 kubeadm.go:394] duration metric: took 15.851570149s to StartCluster
I0905 03:56:40.176811   87328 settings.go:142] acquiring lock: {Name:mke1dd17d08447178171701700c8284a7168ff9a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0905 03:56:40.176917   87328 settings.go:150] Updating kubeconfig:  /home/azureuser/.kube/config
I0905 03:56:40.178459   87328 lock.go:35] WriteFile acquiring /home/azureuser/.kube/config: {Name:mkf9f00f9d22bdb81db643f37edd298a03aef542 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0905 03:56:40.178771   87328 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0905 03:56:40.178776   87328 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0905 03:56:40.178856   87328 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0905 03:56:40.178953   87328 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0905 03:56:40.178972   87328 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0905 03:56:40.179032   87328 host.go:66] Checking if "minikube" exists ...
I0905 03:56:40.179040   87328 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0905 03:56:40.179060   87328 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0905 03:56:40.179081   87328 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0905 03:56:40.179096   87328 addons.go:69] Setting nvidia-device-plugin=true in profile "minikube"
I0905 03:56:40.179112   87328 addons.go:238] Setting addon nvidia-device-plugin=true in "minikube"
I0905 03:56:40.179144   87328 host.go:66] Checking if "minikube" exists ...
I0905 03:56:40.179506   87328 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0905 03:56:40.179680   87328 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0905 03:56:40.179846   87328 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0905 03:56:40.183791   87328 out.go:177] 🔎  Verifying Kubernetes components...
I0905 03:56:40.193587   87328 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0905 03:56:40.225839   87328 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0905 03:56:40.227753   87328 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0905 03:56:40.227793   87328 host.go:66] Checking if "minikube" exists ...
I0905 03:56:40.228940   87328 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0905 03:56:40.233160   87328 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0905 03:56:40.233182   87328 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0905 03:56:40.233354   87328 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0905 03:56:40.236760   87328 out.go:177]     ▪ Using image nvcr.io/nvidia/k8s-device-plugin:v0.17.2
I0905 03:56:40.241458   87328 addons.go:435] installing /etc/kubernetes/addons/nvidia-device-plugin.yaml
I0905 03:56:40.241478   87328 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/nvidia-device-plugin.yaml (1966 bytes)
I0905 03:56:40.241567   87328 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0905 03:56:40.270916   87328 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0905 03:56:40.270934   87328 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0905 03:56:40.271098   87328 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0905 03:56:40.277754   87328 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/azureuser/.minikube/machines/minikube/id_rsa Username:docker}
I0905 03:56:40.288570   87328 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/azureuser/.minikube/machines/minikube/id_rsa Username:docker}
I0905 03:56:40.306375   87328 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/azureuser/.minikube/machines/minikube/id_rsa Username:docker}
I0905 03:56:40.457950   87328 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.58.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0905 03:56:40.710900   87328 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0905 03:56:40.712589   87328 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0905 03:56:40.727561   87328 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0905 03:56:40.829934   87328 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/nvidia-device-plugin.yaml
I0905 03:56:41.454032   87328 start.go:971] {"host.minikube.internal": 192.168.58.1} host record injected into CoreDNS's ConfigMap
I0905 03:56:41.457108   87328 api_server.go:52] waiting for apiserver process to appear ...
I0905 03:56:41.457182   87328 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0905 03:56:41.782426   87328 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.06980347s)
I0905 03:56:41.782480   87328 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.054904819s)
I0905 03:56:41.782839   87328 api_server.go:72] duration metric: took 1.604034175s to wait for apiserver process to appear ...
I0905 03:56:41.782850   87328 api_server.go:88] waiting for apiserver healthz status ...
I0905 03:56:41.782870   87328 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0905 03:56:41.791282   87328 api_server.go:279] https://192.168.58.2:8443/healthz returned 200:
ok
I0905 03:56:41.792505   87328 api_server.go:141] control plane version: v1.33.1
I0905 03:56:41.792521   87328 api_server.go:131] duration metric: took 9.665178ms to wait for apiserver health ...
I0905 03:56:41.792530   87328 system_pods.go:43] waiting for kube-system pods to appear ...
I0905 03:56:41.795208   87328 out.go:177] 🌟  Enabled addons: storage-provisioner, nvidia-device-plugin, default-storageclass
I0905 03:56:41.796377   87328 system_pods.go:59] 5 kube-system pods found
I0905 03:56:41.796399   87328 system_pods.go:61] "etcd-minikube" [dd2449bd-6732-4fe5-96e8-a1ebc6ad6789] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0905 03:56:41.796411   87328 system_pods.go:61] "kube-apiserver-minikube" [613b8908-1784-4fd5-9c41-2b77df3dc2b3] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0905 03:56:41.796421   87328 system_pods.go:61] "kube-controller-manager-minikube" [4083ef83-3500-4680-a51b-862c414492b7] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0905 03:56:41.796429   87328 system_pods.go:61] "kube-scheduler-minikube" [d1e036a8-1859-4145-a1bb-88220d53b453] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0905 03:56:41.796436   87328 system_pods.go:61] "storage-provisioner" [cfda1697-1652-43fb-a78f-34e44e677da6] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0905 03:56:41.796443   87328 system_pods.go:74] duration metric: took 3.906629ms to wait for pod list to return data ...
I0905 03:56:41.796456   87328 kubeadm.go:578] duration metric: took 1.617652481s to wait for: map[apiserver:true system_pods:true]
I0905 03:56:41.796470   87328 node_conditions.go:102] verifying NodePressure condition ...
I0905 03:56:41.799304   87328 node_conditions.go:122] node storage ephemeral capacity is 129886128Ki
I0905 03:56:41.799326   87328 node_conditions.go:123] node cpu capacity is 6
I0905 03:56:41.799339   87328 node_conditions.go:105] duration metric: took 2.864475ms to run NodePressure ...
I0905 03:56:41.799354   87328 start.go:241] waiting for startup goroutines ...
I0905 03:56:41.801413   87328 addons.go:514] duration metric: took 1.622554368s for enable addons: enabled=[storage-provisioner nvidia-device-plugin default-storageclass]
I0905 03:56:41.957444   87328 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0905 03:56:41.957479   87328 start.go:246] waiting for cluster config update ...
I0905 03:56:41.957492   87328 start.go:255] writing updated cluster config ...
I0905 03:56:41.957813   87328 ssh_runner.go:195] Run: rm -f paused
I0905 03:56:42.042889   87328 start.go:607] kubectl: 1.34.0, cluster: 1.33.1 (minor skew: 1)
I0905 03:56:42.048953   87328 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Sep 05 05:32:26 minikube dockerd[1455]: time="2025-09-05T05:32:26.217880953Z" level=info msg="ignoring event" container=e6f698490cb60e51ba74c15abb66969ff5e63aff9ecbab14ba9445377fac86b1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 05:33:24 minikube dockerd[1455]: time="2025-09-05T05:33:24.702069437Z" level=info msg="ignoring event" container=d8dab88085c1864757a673a2821c7c52a8b73e7b107669df37df57a252ce0f64 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 05:35:01 minikube dockerd[1455]: time="2025-09-05T05:35:01.795503642Z" level=info msg="ignoring event" container=8214bbf029304cae74f990788e8a7bcdd86bef72d69d16859f97d38bf4ee9f8f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 05:38:20 minikube dockerd[1455]: time="2025-09-05T05:38:20.094876141Z" level=info msg="ignoring event" container=9d1a5fc4d6c26d7632ae98783461501aa97c44f830ce6e687f59badc1f96f495 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 05:43:52 minikube dockerd[1455]: time="2025-09-05T05:43:52.426100831Z" level=info msg="ignoring event" container=148b3a883a253fe7454048a22758b009093cc54dce67e2248d1f1c3409c2201b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 05:49:26 minikube dockerd[1455]: time="2025-09-05T05:49:26.968600606Z" level=info msg="ignoring event" container=6559756330afd72e05f5fe4ae767decb01bc76eac93a72e899f68bd2e7e5ca37 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 05:53:41 minikube dockerd[1455]: time="2025-09-05T05:53:41.016890465Z" level=info msg="ignoring event" container=b558cb308b447450f6b8d3e6249ce84db18cc651cedfc0df1465b5246180f48a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 05:54:46 minikube dockerd[1455]: time="2025-09-05T05:54:46.741448788Z" level=info msg="ignoring event" container=1c9eaeb2db3ca98569bc9cd297fbef3ccd9fac443e01f4b082fe430985611021 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:00:21 minikube dockerd[1455]: time="2025-09-05T06:00:21.614368594Z" level=info msg="ignoring event" container=bc0b5c79c125191dc7ff8e74a5f740d4ee3d61ec3457c40be39e761b1279b826 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:03:10 minikube dockerd[1455]: time="2025-09-05T06:03:10.104334082Z" level=info msg="ignoring event" container=02614962122dca9a187f0d42275232c2853c3dae9862ed901b5c6cc097270305 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:05:20 minikube dockerd[1455]: time="2025-09-05T06:05:20.930530240Z" level=info msg="ignoring event" container=f565d2ff607203bf7379d7220e12080464c9a1d6ce8bd872c6dd6f0a2b7fe67b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:05:21 minikube dockerd[1455]: time="2025-09-05T06:05:21.211401257Z" level=info msg="ignoring event" container=9fa0a41a82d900d27142c138e53149a073bc24990b4e445c2651738ec9c656ec module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:05:22 minikube cri-dockerd[1773]: time="2025-09-05T06:05:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9c576eb7c19cdb03298666b5037931f4555157c58af0836a705358a8507911fb/resolv.conf as [nameserver 10.96.0.10 search mnist-gpu-demo.svc.cluster.local svc.cluster.local cluster.local otnvegtkq2zujdq1a1axahua4e.xx.internal.cloudapp.net options ndots:5]"
Sep 05 06:05:57 minikube dockerd[1455]: time="2025-09-05T06:05:57.331213607Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/1b031baf6504bf543222c1d06d92b8ab3a2312ea1941f88534308be3a8a83d1a/merged/tmp/tempfile-mnist-gpu-cf678c46f-hbcmv-9905abbc-1-63e07a013b095\": lstat /var/lib/docker/overlay2/1b031baf6504bf543222c1d06d92b8ab3a2312ea1941f88534308be3a8a83d1a/merged/tmp/tempfile-mnist-gpu-cf678c46f-hbcmv-9905abbc-1-63e07a013b095: no such file or directory"
Sep 05 06:05:57 minikube dockerd[1455]: time="2025-09-05T06:05:57.939746957Z" level=info msg="ignoring event" container=03729b13f4c13ac5d44102531e352e1b938c8fd2824995c9ad824ac8655b92ec module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:06:41 minikube dockerd[1455]: time="2025-09-05T06:06:41.723736257Z" level=info msg="ignoring event" container=5eaf2c959546785edd5d3487ad00de043987f1af23511c45a1c24d589fb39853 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:08:32 minikube dockerd[1455]: time="2025-09-05T06:08:32.114285849Z" level=info msg="ignoring event" container=31251589b9c7625212979c6c2328f58bcaabdeddd23f51765809b7b3611adc11 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:08:47 minikube dockerd[1455]: time="2025-09-05T06:08:47.851031201Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/a6b71feefd62da3b18c9d24389f66f35b522c2ba74f62b630efc0ea75d65af03/merged/tmp/__autograph_generated_file7c8ngrbf.py\": lstat /var/lib/docker/overlay2/a6b71feefd62da3b18c9d24389f66f35b522c2ba74f62b630efc0ea75d65af03/merged/tmp/__autograph_generated_file7c8ngrbf.py: no such file or directory"
Sep 05 06:08:47 minikube dockerd[1455]: time="2025-09-05T06:08:47.851140096Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/a6b71feefd62da3b18c9d24389f66f35b522c2ba74f62b630efc0ea75d65af03/merged/tmp/__autograph_generated_fileakpohabt.py\": lstat /var/lib/docker/overlay2/a6b71feefd62da3b18c9d24389f66f35b522c2ba74f62b630efc0ea75d65af03/merged/tmp/__autograph_generated_fileakpohabt.py: no such file or directory"
Sep 05 06:08:47 minikube dockerd[1455]: time="2025-09-05T06:08:47.851184794Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/a6b71feefd62da3b18c9d24389f66f35b522c2ba74f62b630efc0ea75d65af03/merged/tmp/__autograph_generated_fileoen6l70k.py\": lstat /var/lib/docker/overlay2/a6b71feefd62da3b18c9d24389f66f35b522c2ba74f62b630efc0ea75d65af03/merged/tmp/__autograph_generated_fileoen6l70k.py: no such file or directory"
Sep 05 06:08:47 minikube dockerd[1455]: time="2025-09-05T06:08:47.851290389Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/a6b71feefd62da3b18c9d24389f66f35b522c2ba74f62b630efc0ea75d65af03/merged/tmp/__autograph_generated_filep0z3akdg.py\": lstat /var/lib/docker/overlay2/a6b71feefd62da3b18c9d24389f66f35b522c2ba74f62b630efc0ea75d65af03/merged/tmp/__autograph_generated_filep0z3akdg.py: no such file or directory"
Sep 05 06:11:13 minikube dockerd[1455]: time="2025-09-05T06:11:13.231692102Z" level=info msg="ignoring event" container=f11fb31c173ff08ad79d8fb85da6907157897231ff8ff4db450db9cff5fe6a3e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:15:20 minikube dockerd[1455]: time="2025-09-05T06:15:20.916259994Z" level=info msg="ignoring event" container=1b1744a63eaa25e149c27e2d11b2a0b2fb84f51075cf45dab32dc33f1c544f42 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:15:38 minikube dockerd[1455]: time="2025-09-05T06:15:38.313814349Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/3e1b226cd561c3d5ad37d774be87988e03df52f737683661b90a25716a952cfa/merged/tmp/__autograph_generated_filex_6wz9tn.py\": lstat /var/lib/docker/overlay2/3e1b226cd561c3d5ad37d774be87988e03df52f737683661b90a25716a952cfa/merged/tmp/__autograph_generated_filex_6wz9tn.py: no such file or directory"
Sep 05 06:15:38 minikube dockerd[1455]: time="2025-09-05T06:15:38.317583384Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/3e1b226cd561c3d5ad37d774be87988e03df52f737683661b90a25716a952cfa/merged/tmp/__autograph_generated_filef0ppkkai.py\": lstat /var/lib/docker/overlay2/3e1b226cd561c3d5ad37d774be87988e03df52f737683661b90a25716a952cfa/merged/tmp/__autograph_generated_filef0ppkkai.py: no such file or directory"
Sep 05 06:21:08 minikube dockerd[1455]: time="2025-09-05T06:21:07.951806258Z" level=info msg="ignoring event" container=b0e926d909589d3cdc4abaa12783849b9d86ef797493cbe316e3dcf78140f30e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:21:15 minikube cri-dockerd[1773]: time="2025-09-05T06:21:15Z" level=error msg="error collecting stats for container 'mnist-gpu': Error response from daemon: No such container: 1b1744a63eaa25e149c27e2d11b2a0b2fb84f51075cf45dab32dc33f1c544f42"
Sep 05 06:21:32 minikube dockerd[1455]: time="2025-09-05T06:21:32.408102890Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/3491921eaec68921d2ccc576a32c044b1c33429f5456c3c4f97d59d22d04803f/merged/tmp/__autograph_generated_filey74u8z52.py\": lstat /var/lib/docker/overlay2/3491921eaec68921d2ccc576a32c044b1c33429f5456c3c4f97d59d22d04803f/merged/tmp/__autograph_generated_filey74u8z52.py: no such file or directory"
Sep 05 06:21:32 minikube dockerd[1455]: time="2025-09-05T06:21:32.408909256Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/3491921eaec68921d2ccc576a32c044b1c33429f5456c3c4f97d59d22d04803f/merged/tmp/tempfile-mnist-gpu-cf678c46f-hbcmv-8c2d6adb-1-63e07d31fb958\": lstat /var/lib/docker/overlay2/3491921eaec68921d2ccc576a32c044b1c33429f5456c3c4f97d59d22d04803f/merged/tmp/tempfile-mnist-gpu-cf678c46f-hbcmv-8c2d6adb-1-63e07d31fb958: no such file or directory"
Sep 05 06:21:32 minikube dockerd[1455]: time="2025-09-05T06:21:32.411125962Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/3491921eaec68921d2ccc576a32c044b1c33429f5456c3c4f97d59d22d04803f/merged/tmp/__autograph_generated_filei3_owxd8.py\": lstat /var/lib/docker/overlay2/3491921eaec68921d2ccc576a32c044b1c33429f5456c3c4f97d59d22d04803f/merged/tmp/__autograph_generated_filei3_owxd8.py: no such file or directory"
Sep 05 06:25:55 minikube cri-dockerd[1773]: time="2025-09-05T06:25:55Z" level=error msg="error getting RW layer size for container ID '11a35495f1d6d752c86d1a1f4e832a33c44a9af466d7b0f70e1d166a1b913720': operation timeout: context deadline exceeded"
Sep 05 06:25:55 minikube cri-dockerd[1773]: time="2025-09-05T06:25:55Z" level=error msg="Set backoffDuration to : 2m0s for container ID '11a35495f1d6d752c86d1a1f4e832a33c44a9af466d7b0f70e1d166a1b913720'"
Sep 05 06:26:03 minikube dockerd[1455]: time="2025-09-05T06:26:03.656928291Z" level=error msg="Handler for GET /v1.46/containers/11a35495f1d6d752c86d1a1f4e832a33c44a9af466d7b0f70e1d166a1b913720/json returned error: write unix /var/run/docker.sock->@: write: broken pipe"
Sep 05 06:26:03 minikube dockerd[1455]: 2025/09/05 06:26:03 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/internal/request.(*RespWriterWrapper).writeHeader (resp_writer_wrapper.go:80)
Sep 05 06:26:25 minikube cri-dockerd[1773]: time="2025-09-05T06:26:25Z" level=error msg="error getting RW layer size for container ID 'b0e926d909589d3cdc4abaa12783849b9d86ef797493cbe316e3dcf78140f30e': operation timeout: context deadline exceeded"
Sep 05 06:26:25 minikube cri-dockerd[1773]: time="2025-09-05T06:26:25Z" level=error msg="Set backoffDuration to : 2m0s for container ID 'b0e926d909589d3cdc4abaa12783849b9d86ef797493cbe316e3dcf78140f30e'"
Sep 05 06:26:56 minikube dockerd[1455]: time="2025-09-05T06:26:56.903488497Z" level=error msg="Handler for GET /v1.46/containers/b0e926d909589d3cdc4abaa12783849b9d86ef797493cbe316e3dcf78140f30e/json returned error: write unix /var/run/docker.sock->@: write: broken pipe"
Sep 05 06:26:56 minikube dockerd[1455]: 2025/09/05 06:26:56 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/internal/request.(*RespWriterWrapper).writeHeader (resp_writer_wrapper.go:80)
Sep 05 06:27:53 minikube dockerd[1455]: time="2025-09-05T06:27:53.038430245Z" level=info msg="ignoring event" container=11a35495f1d6d752c86d1a1f4e832a33c44a9af466d7b0f70e1d166a1b913720 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:35:01 minikube cri-dockerd[1773]: time="2025-09-05T06:35:01Z" level=error msg="error getting RW layer size for container ID 'c130930be637a544ea0e082f8c0bf1d3db5516d70583a3a8c9974dafba491143': operation timeout: context deadline exceeded"
Sep 05 06:35:01 minikube cri-dockerd[1773]: time="2025-09-05T06:35:01Z" level=error msg="Set backoffDuration to : 2m0s for container ID 'c130930be637a544ea0e082f8c0bf1d3db5516d70583a3a8c9974dafba491143'"
Sep 05 06:35:11 minikube dockerd[1455]: time="2025-09-05T06:35:11.419952347Z" level=error msg="Handler for GET /v1.46/containers/c130930be637a544ea0e082f8c0bf1d3db5516d70583a3a8c9974dafba491143/json returned error: write unix /var/run/docker.sock->@: write: broken pipe"
Sep 05 06:35:11 minikube dockerd[1455]: 2025/09/05 06:35:11 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/internal/request.(*RespWriterWrapper).writeHeader (resp_writer_wrapper.go:80)
Sep 05 06:35:38 minikube cri-dockerd[1773]: time="2025-09-05T06:35:38Z" level=error msg="error getting RW layer size for container ID '11a35495f1d6d752c86d1a1f4e832a33c44a9af466d7b0f70e1d166a1b913720': operation timeout: context deadline exceeded"
Sep 05 06:35:38 minikube cri-dockerd[1773]: time="2025-09-05T06:35:38Z" level=error msg="Set backoffDuration to : 2m0s for container ID '11a35495f1d6d752c86d1a1f4e832a33c44a9af466d7b0f70e1d166a1b913720'"
Sep 05 06:35:48 minikube dockerd[1455]: time="2025-09-05T06:35:48.202593866Z" level=error msg="Handler for GET /v1.46/containers/11a35495f1d6d752c86d1a1f4e832a33c44a9af466d7b0f70e1d166a1b913720/json returned error: write unix /var/run/docker.sock->@: write: broken pipe"
Sep 05 06:35:48 minikube dockerd[1455]: 2025/09/05 06:35:48 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/internal/request.(*RespWriterWrapper).writeHeader (resp_writer_wrapper.go:80)
Sep 05 06:36:41 minikube dockerd[1455]: time="2025-09-05T06:36:41.715348862Z" level=info msg="ignoring event" container=c130930be637a544ea0e082f8c0bf1d3db5516d70583a3a8c9974dafba491143 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:44:14 minikube dockerd[1455]: time="2025-09-05T06:44:14.036115652Z" level=info msg="ignoring event" container=77a371de7935271d63839480f2f1e58b16288183e6faca02608b6159d287021e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:44:18 minikube dockerd[1455]: time="2025-09-05T06:44:18.908201514Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/3469ba5be00d03f84ff87290c1b7e6f2a7e350de016333939720e66b9e9e0384/merged/tmp/__autograph_generated_file1ykpwhdb.py\": lstat /var/lib/docker/overlay2/3469ba5be00d03f84ff87290c1b7e6f2a7e350de016333939720e66b9e9e0384/merged/tmp/__autograph_generated_file1ykpwhdb.py: no such file or directory"
Sep 05 06:44:18 minikube dockerd[1455]: time="2025-09-05T06:44:18.908274711Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/3469ba5be00d03f84ff87290c1b7e6f2a7e350de016333939720e66b9e9e0384/merged/tmp/__autograph_generated_fileasdjn1ma.py\": lstat /var/lib/docker/overlay2/3469ba5be00d03f84ff87290c1b7e6f2a7e350de016333939720e66b9e9e0384/merged/tmp/__autograph_generated_fileasdjn1ma.py: no such file or directory"
Sep 05 06:49:40 minikube dockerd[1455]: time="2025-09-05T06:49:40.040718095Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/31b64192d58af01b705a4dd133f2050540f7ec306d5cb671cfb607e3742b8df2/merged/tmp/__autograph_generated_file3xxywsf0.py\": lstat /var/lib/docker/overlay2/31b64192d58af01b705a4dd133f2050540f7ec306d5cb671cfb607e3742b8df2/merged/tmp/__autograph_generated_file3xxywsf0.py: no such file or directory"
Sep 05 06:49:40 minikube dockerd[1455]: time="2025-09-05T06:49:40.040810891Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/31b64192d58af01b705a4dd133f2050540f7ec306d5cb671cfb607e3742b8df2/merged/tmp/__autograph_generated_fileo45joerm.py\": lstat /var/lib/docker/overlay2/31b64192d58af01b705a4dd133f2050540f7ec306d5cb671cfb607e3742b8df2/merged/tmp/__autograph_generated_fileo45joerm.py: no such file or directory"
Sep 05 06:49:40 minikube dockerd[1455]: time="2025-09-05T06:49:40.040862788Z" level=error msg="Can not stat \"/var/lib/docker/overlay2/31b64192d58af01b705a4dd133f2050540f7ec306d5cb671cfb607e3742b8df2/merged/tmp/tempfile-mnist-gpu-cf678c46f-hbcmv-cd1b6b14-1-63e083dc19918\": lstat /var/lib/docker/overlay2/31b64192d58af01b705a4dd133f2050540f7ec306d5cb671cfb607e3742b8df2/merged/tmp/tempfile-mnist-gpu-cf678c46f-hbcmv-cd1b6b14-1-63e083dc19918: no such file or directory"
Sep 05 06:49:40 minikube dockerd[1455]: time="2025-09-05T06:49:40.578893707Z" level=info msg="ignoring event" container=8ff3d98ea70ac95f709343ff7313c31c47b9a2c841b2d81fa5dc9cea44a59d81 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:55:04 minikube dockerd[1455]: time="2025-09-05T06:55:04.928874827Z" level=info msg="ignoring event" container=b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 06:58:02 minikube dockerd[1455]: time="2025-09-05T06:58:02.205773000Z" level=info msg="ignoring event" container=fe3b3c9de0047c8de319be8d9bcc3a3a067e49d3a1b7ace744bc2fc1a33fa8d3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 07:00:26 minikube dockerd[1455]: time="2025-09-05T07:00:26.193395691Z" level=info msg="ignoring event" container=0862aaf3f2710370a1d1c3ef4deba89df223f6a4d1c67c834003ba3ba237095e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 05 07:00:31 minikube cri-dockerd[1773]: time="2025-09-05T07:00:31Z" level=error msg="error getting RW layer size for container ID 'b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a': Error response from daemon: No such container: b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 07:00:31 minikube cri-dockerd[1773]: time="2025-09-05T07:00:31Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a'"


==> container status <==
CONTAINER           IMAGE                                                                                                      CREATED              STATE               NAME                       ATTEMPT             POD ID              POD
0862aaf3f2710       aa93e6c2f7f98                                                                                              44 seconds ago       Exited              mnist-gpu                  11                  9c576eb7c19cd       mnist-gpu-cf678c46f-hbcmv
580594a4c8f2e       6e38f40d628db                                                                                              About a minute ago   Running             storage-provisioner        6                   214b1defee07b       storage-provisioner
fe3b3c9de0047       6e38f40d628db                                                                                              57 minutes ago       Exited              storage-provisioner        5                   214b1defee07b       storage-provisioner
f5aeaf4ff51ad       nvcr.io/nvidia/k8s-device-plugin@sha256:037160a36de0f060fc21cc0cb2f795d980282ff1471b55530433ca4350b24c4f   3 hours ago          Running             nvidia-device-plugin-ctr   0                   15b91addeeaa6       nvidia-device-plugin-daemonset-ds2s5
46f1257385456       b79c189b052cd                                                                                              3 hours ago          Running             kube-proxy                 0                   be5f6b3c53e36       kube-proxy-7cz4p
c1d1403b6d7a0       1cf5f116067c6                                                                                              3 hours ago          Running             coredns                    0                   571bf8274c5e0       coredns-674b8bbfcf-db477
89d34111aadf6       398c985c0d950                                                                                              3 hours ago          Running             kube-scheduler             0                   ea9729a98490c       kube-scheduler-minikube
6d9c89c82e47d       499038711c081                                                                                              3 hours ago          Running             etcd                       0                   6f8d9e512e0d5       etcd-minikube
86ce0bfab5c4a       ef43894fa110c                                                                                              3 hours ago          Running             kube-controller-manager    0                   e6b11db4d29b4       kube-controller-manager-minikube
973f949e777e0       c6ab243b29f82                                                                                              3 hours ago          Running             kube-apiserver             0                   916bd6da03ffe       kube-apiserver-minikube


==> coredns [c1d1403b6d7a] <==
[INFO] 10.244.0.13:49146 - 63193 "A IN storage.googleapis.com.mnist-gpu-demo.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.002649984s
[INFO] 10.244.0.13:49146 - 24521 "AAAA IN storage.googleapis.com.mnist-gpu-demo.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.00477359s
[INFO] 10.244.0.13:34745 - 9421 "AAAA IN storage.googleapis.com.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.001134151s
[INFO] 10.244.0.13:34745 - 41535 "A IN storage.googleapis.com.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.001098652s
[INFO] 10.244.0.13:53540 - 63389 "AAAA IN storage.googleapis.com.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.001058554s
[INFO] 10.244.0.13:53540 - 5527 "A IN storage.googleapis.com.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.048429076s
[INFO] 10.244.0.13:44128 - 22666 "A IN storage.googleapis.com.otnvegtkq2zujdq1a1axahua4e.xx.internal.cloudapp.net. udp 92 false 512" NXDOMAIN qr,rd,ra 202 0.020200815s
[INFO] 10.244.0.13:44128 - 8121 "AAAA IN storage.googleapis.com.otnvegtkq2zujdq1a1axahua4e.xx.internal.cloudapp.net. udp 92 false 512" NXDOMAIN qr,rd,ra 202 0.014341172s
[INFO] 10.244.0.13:39248 - 33490 "AAAA IN storage.googleapis.com. udp 40 false 512" NOERROR qr,rd,ra 240 0.017935613s
[INFO] 10.244.0.13:39248 - 7135 "A IN storage.googleapis.com. udp 40 false 512" NOERROR qr,rd,ra 344 0.020547699s
[INFO] 10.244.0.13:45520 - 53015 "A IN storage.googleapis.com.mnist-gpu-demo.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.006129629s
[INFO] 10.244.0.13:45520 - 57380 "AAAA IN storage.googleapis.com.mnist-gpu-demo.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.001105851s
[INFO] 10.244.0.13:48276 - 9023 "A IN storage.googleapis.com.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000970657s
[INFO] 10.244.0.13:48276 - 2570 "AAAA IN storage.googleapis.com.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.002407094s
[INFO] 10.244.0.13:60241 - 63555 "A IN storage.googleapis.com.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.001024554s
[INFO] 10.244.0.13:60241 - 16471 "AAAA IN storage.googleapis.com.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.004655894s
[INFO] 10.244.0.13:60415 - 31729 "A IN storage.googleapis.com.otnvegtkq2zujdq1a1axahua4e.xx.internal.cloudapp.net. udp 92 false 512" NXDOMAIN qr,rd,ra 202 0.019240049s
[INFO] 10.244.0.13:60415 - 64646 "AAAA IN storage.googleapis.com.otnvegtkq2zujdq1a1axahua4e.xx.internal.cloudapp.net. udp 92 false 512" NXDOMAIN qr,rd,ra 202 0.019680628s
[INFO] 10.244.0.13:56189 - 12671 "A IN storage.googleapis.com. udp 40 false 512" NOERROR qr,rd,ra 344 0.066877339s
[INFO] 10.244.0.13:56189 - 37737 "AAAA IN storage.googleapis.com. udp 40 false 512" NOERROR qr,rd,ra 240 0.06800389s
[INFO] 10.244.0.13:60556 - 57508 "A IN storage.googleapis.com.mnist-gpu-demo.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000451681s
[INFO] 10.244.0.13:60556 - 16472 "AAAA IN storage.googleapis.com.mnist-gpu-demo.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000616974s
[INFO] 10.244.0.13:59147 - 48170 "AAAA IN storage.googleapis.com.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000289087s
[INFO] 10.244.0.13:59147 - 3873 "A IN storage.googleapis.com.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000395282s
[INFO] 10.244.0.13:33187 - 7571 "AAAA IN storage.googleapis.com.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000270688s
[INFO] 10.244.0.13:33187 - 22425 "A IN storage.googleapis.com.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000366484s
[INFO] 10.244.0.13:53285 - 54653 "AAAA IN storage.googleapis.com.otnvegtkq2zujdq1a1axahua4e.xx.internal.cloudapp.net. udp 92 false 512" NXDOMAIN qr,rd,ra 202 0.008285138s
[INFO] 10.244.0.13:53285 - 63845 "A IN storage.googleapis.com.otnvegtkq2zujdq1a1axahua4e.xx.internal.cloudapp.net. udp 92 false 512" NXDOMAIN qr,rd,ra 202 0.009165s
[INFO] 10.244.0.13:54156 - 26766 "AAAA IN storage.googleapis.com. udp 40 false 512" NOERROR qr,rd,ra 240 0.008592525s
[INFO] 10.244.0.13:54156 - 1941 "A IN storage.googleapis.com. udp 40 false 512" NOERROR qr,rd,ra 344 0.008649522s
[INFO] 10.244.0.13:60685 - 7186 "AAAA IN storage.googleapis.com.mnist-gpu-demo.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000368383s
[INFO] 10.244.0.13:60685 - 22554 "A IN storage.googleapis.com.mnist-gpu-demo.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000177892s
[INFO] 10.244.0.13:54489 - 31642 "A IN storage.googleapis.com.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000179892s
[INFO] 10.244.0.13:54489 - 39324 "AAAA IN storage.googleapis.com.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000256789s
[INFO] 10.244.0.13:40284 - 43267 "AAAA IN storage.googleapis.com.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000184892s
[INFO] 10.244.0.13:40284 - 34830 "A IN storage.googleapis.com.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000253689s
[INFO] 10.244.0.13:40642 - 40117 "AAAA IN storage.googleapis.com.otnvegtkq2zujdq1a1axahua4e.xx.internal.cloudapp.net. udp 92 false 512" NXDOMAIN qr,rd,ra 202 0.005232272s
[INFO] 10.244.0.13:40642 - 11964 "A IN storage.googleapis.com.otnvegtkq2zujdq1a1axahua4e.xx.internal.cloudapp.net. udp 92 false 512" NXDOMAIN qr,rd,ra 202 0.005321168s
[INFO] 10.244.0.13:56818 - 35502 "AAAA IN storage.googleapis.com. udp 40 false 512" NOERROR qr,rd,ra 240 0.002739281s
[INFO] 10.244.0.13:56818 - 53922 "A IN storage.googleapis.com. udp 40 false 512" NOERROR qr,rd,ra 344 0.002832476s
[INFO] 10.244.0.13:36620 - 15270 "AAAA IN storage.googleapis.com.mnist-gpu-demo.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000295087s
[INFO] 10.244.0.13:36620 - 32347 "A IN storage.googleapis.com.mnist-gpu-demo.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.00043638s
[INFO] 10.244.0.13:58590 - 6588 "AAAA IN storage.googleapis.com.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000153693s
[INFO] 10.244.0.13:58590 - 15545 "A IN storage.googleapis.com.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00023009s
[INFO] 10.244.0.13:53193 - 30803 "AAAA IN storage.googleapis.com.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000149893s
[INFO] 10.244.0.13:53193 - 16815 "A IN storage.googleapis.com.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000214991s
[INFO] 10.244.0.13:50891 - 15932 "A IN storage.googleapis.com.otnvegtkq2zujdq1a1axahua4e.xx.internal.cloudapp.net. udp 92 false 512" NXDOMAIN qr,rd,ra 202 0.004366603s
[INFO] 10.244.0.13:50891 - 54587 "AAAA IN storage.googleapis.com.otnvegtkq2zujdq1a1axahua4e.xx.internal.cloudapp.net. udp 92 false 512" NXDOMAIN qr,rd,ra 202 0.004765885s
[INFO] 10.244.0.13:48781 - 20313 "A IN storage.googleapis.com. udp 40 false 512" NOERROR qr,rd,ra 344 0.002061207s
[INFO] 10.244.0.13:48781 - 36444 "AAAA IN storage.googleapis.com. udp 40 false 512" NOERROR qr,rd,ra 240 0.002173702s
[INFO] 10.244.0.13:54427 - 61241 "AAAA IN storage.googleapis.com.mnist-gpu-demo.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000371084s
[INFO] 10.244.0.13:54427 - 32561 "A IN storage.googleapis.com.mnist-gpu-demo.svc.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000353585s
[INFO] 10.244.0.13:35531 - 37505 "AAAA IN storage.googleapis.com.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000298887s
[INFO] 10.244.0.13:35531 - 390 "A IN storage.googleapis.com.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00044468s
[INFO] 10.244.0.13:46928 - 26740 "AAAA IN storage.googleapis.com.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000213991s
[INFO] 10.244.0.13:46928 - 22399 "A IN storage.googleapis.com.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.00022989s
[INFO] 10.244.0.13:47504 - 42030 "AAAA IN storage.googleapis.com.otnvegtkq2zujdq1a1axahua4e.xx.internal.cloudapp.net. udp 92 false 512" NXDOMAIN qr,rd,ra 202 0.004732592s
[INFO] 10.244.0.13:47504 - 12326 "A IN storage.googleapis.com.otnvegtkq2zujdq1a1axahua4e.xx.internal.cloudapp.net. udp 92 false 512" NXDOMAIN qr,rd,ra 202 0.004849488s
[INFO] 10.244.0.13:53093 - 14586 "A IN storage.googleapis.com. udp 40 false 512" NOERROR qr,rd,ra 344 0.00295187s
[INFO] 10.244.0.13:53093 - 46591 "AAAA IN storage.googleapis.com. udp 40 false 512" NOERROR qr,rd,ra 240 0.003128463s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_09_05T03_56_39_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 05 Sep 2025 03:56:36 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 05 Sep 2025 07:00:51 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 05 Sep 2025 06:55:54 +0000   Fri, 05 Sep 2025 03:56:33 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 05 Sep 2025 06:55:54 +0000   Fri, 05 Sep 2025 03:56:33 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 05 Sep 2025 06:55:54 +0000   Fri, 05 Sep 2025 03:56:33 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 05 Sep 2025 06:55:54 +0000   Fri, 05 Sep 2025 03:56:36 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    minikube
Capacity:
  cpu:                6
  ephemeral-storage:  129886128Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             115389724Ki
  nvidia.com/gpu:     1
  pods:               110
Allocatable:
  cpu:                6
  ephemeral-storage:  129886128Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             115389724Ki
  nvidia.com/gpu:     1
  pods:               110
System Info:
  Machine ID:                 f80204b67bbb49a380c853a752655570
  System UUID:                3f3dcd24-8305-4c8f-82b8-5dcb68e4887c
  Boot ID:                    f016d6fe-fccb-4a1f-a7ce-0882bd4e57c6
  Kernel Version:             6.8.0-1026-azure
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-674b8bbfcf-db477                100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     3h4m
  kube-system                 etcd-minikube                           100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         3h4m
  kube-system                 kube-apiserver-minikube                 250m (4%)     0 (0%)      0 (0%)           0 (0%)         3h4m
  kube-system                 kube-controller-manager-minikube        200m (3%)     0 (0%)      0 (0%)           0 (0%)         3h4m
  kube-system                 kube-proxy-7cz4p                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h4m
  kube-system                 kube-scheduler-minikube                 100m (1%)     0 (0%)      0 (0%)           0 (0%)         3h4m
  kube-system                 nvidia-device-plugin-daemonset-ds2s5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h4m
  kube-system                 storage-provisioner                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h4m
  mnist-gpu-demo              mnist-gpu-cf678c46f-hbcmv               0 (0%)        0 (0%)      0 (0%)           0 (0%)         55m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (12%)  0 (0%)
  memory             170Mi (0%)  170Mi (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
  nvidia.com/gpu     1           1
Events:              <none>


==> dmesg <==
[Sep 4 20:11] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended configuration space under this bridge
[  +0.026711] * Found PM-Timer Bug on the chipset. Due to workarounds for a bug,
              * this clock source is slow. Consider trying other clock sources
[  +0.692601] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000087] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +1.035451] block sdb: the capability attribute has been deprecated.
[  +0.060237] systemd[1]: Configuration file /run/systemd/system/netplan-ovs-cleanup.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
[  +0.024505] systemd[1]: /lib/systemd/system/snapd.service:23: Unknown key name 'RestartMode' in section 'Service', ignoring.
[  +0.648712] systemd-journald[185]: File /var/log/journal/4e10be25ce2346e3a945ee0494972293/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.639884] nvidia: loading out-of-tree module taints kernel.
[  +0.000010] nvidia: module license 'NVIDIA' taints kernel.
[  +0.000001] Disabling lock debugging due to kernel taint
[  +0.000004] nvidia: module license taints kernel.

[  +0.758665] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  550.90.07  Fri May 31 09:35:42 UTC 2024
[  +0.228774] workqueue: drm_fb_helper_damage_work hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +1.516636] /dev/sr0: Can't lookup blockdev
[Sep 4 20:12] workqueue: drm_fb_helper_damage_work hogged CPU for >10000us 8 times, consider switching to WQ_UNBOUND
[  +6.512543] nvidia_uvm: module uses symbols nvUvmInterfaceDisableAccessCntr from proprietary module nvidia, inheriting taint.
[Sep 4 20:47] hrtimer: interrupt took 2659784 ns


==> etcd [6d9c89c82e47] <==
{"level":"warn","ts":"2025-09-05T06:59:23.586194Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.393722826s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-05T06:59:23.586221Z","caller":"traceutil/trace.go:171","msg":"trace[1749160306] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:9489; }","duration":"1.393760224s","start":"2025-09-05T06:59:22.192452Z","end":"2025-09-05T06:59:23.586212Z","steps":["trace[1749160306] 'agreement among raft nodes before linearized reading'  (duration: 1.393705726s)"],"step_count":1}
{"level":"warn","ts":"2025-09-05T06:59:24.087670Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238537935495157230,"retry-timeout":"500ms"}
{"level":"warn","ts":"2025-09-05T06:59:24.199677Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.000326383s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"","error":"context deadline exceeded"}
{"level":"info","ts":"2025-09-05T06:59:24.199774Z","caller":"traceutil/trace.go:171","msg":"trace[308608509] range","detail":"{range_begin:/registry/health; range_end:; }","duration":"2.000458977s","start":"2025-09-05T06:59:22.199293Z","end":"2025-09-05T06:59:24.199752Z","steps":["trace[308608509] 'agreement among raft nodes before linearized reading'  (duration: 2.000340882s)"],"step_count":1}
{"level":"warn","ts":"2025-09-05T06:59:24.199832Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:22.199284Z","time spent":"2.000536074s","remote":"127.0.0.1:49848","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":0,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-09-05T06:59:24.588471Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238537935495157230,"retry-timeout":"500ms"}
{"level":"warn","ts":"2025-09-05T06:59:25.089583Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238537935495157230,"retry-timeout":"500ms"}
{"level":"warn","ts":"2025-09-05T06:59:25.224561Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.701589034s","expected-duration":"1s"}
{"level":"warn","ts":"2025-09-05T06:59:26.039623Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"814.780327ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238537935495157229 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:9486 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >>","response":"size:16"}
{"level":"info","ts":"2025-09-05T06:59:26.039747Z","caller":"traceutil/trace.go:171","msg":"trace[2083430463] transaction","detail":"{read_only:false; response_revision:9490; number_of_response:1; }","duration":"2.516910537s","start":"2025-09-05T06:59:23.522816Z","end":"2025-09-05T06:59:26.039727Z","steps":["trace[2083430463] 'process raft request'  (duration: 1.701949319s)","trace[2083430463] 'compare'  (duration: 814.47184ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-05T06:59:26.039827Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:23.522788Z","time spent":"2.517003333s","remote":"127.0.0.1:50270","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:9486 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2025-09-05T06:59:26.057481Z","caller":"traceutil/trace.go:171","msg":"trace[1906264277] linearizableReadLoop","detail":"{readStateIndex:11794; appliedIndex:11792; }","duration":"2.471331956s","start":"2025-09-05T06:59:23.586134Z","end":"2025-09-05T06:59:26.057466Z","steps":["trace[1906264277] 'read index received'  (duration: 1.638616621s)","trace[1906264277] 'applied index is now lower than readState.Index'  (duration: 832.714535ms)"],"step_count":2}
{"level":"info","ts":"2025-09-05T06:59:26.057521Z","caller":"traceutil/trace.go:171","msg":"trace[658009114] transaction","detail":"{read_only:false; response_revision:9491; number_of_response:1; }","duration":"2.47056079s","start":"2025-09-05T06:59:23.586927Z","end":"2025-09-05T06:59:26.057488Z","steps":["trace[658009114] 'process raft request'  (duration: 2.470411697s)"],"step_count":1}
{"level":"warn","ts":"2025-09-05T06:59:26.057622Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:23.586910Z","time spent":"2.470656586s","remote":"127.0.0.1:49864","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.58.2\" mod_revision:0 > success:<request_put:<key:\"/registry/masterleases/192.168.58.2\" value_size:65 lease:3238537935495157227 >> failure:<request_range:<key:\"/registry/masterleases/192.168.58.2\" > >"}
{"level":"warn","ts":"2025-09-05T06:59:26.057694Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"3.858915054s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/kube-apiserver-minikube\" limit:1 ","response":"range_response_count:1 size:7676"}
{"level":"info","ts":"2025-09-05T06:59:26.057808Z","caller":"traceutil/trace.go:171","msg":"trace[927530181] range","detail":"{range_begin:/registry/pods/kube-system/kube-apiserver-minikube; range_end:; response_count:1; response_revision:9491; }","duration":"3.859058247s","start":"2025-09-05T06:59:22.198739Z","end":"2025-09-05T06:59:26.057797Z","steps":["trace[927530181] 'agreement among raft nodes before linearized reading'  (duration: 3.858828857s)"],"step_count":1}
{"level":"warn","ts":"2025-09-05T06:59:26.057858Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:22.198726Z","time spent":"3.859119444s","remote":"127.0.0.1:50194","response type":"/etcdserverpb.KV/Range","request count":0,"request size":54,"response count":1,"response size":7699,"request content":"key:\"/registry/pods/kube-system/kube-apiserver-minikube\" limit:1 "}
{"level":"warn","ts":"2025-09-05T06:59:26.201361Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.000825091s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"","error":"context canceled"}
{"level":"info","ts":"2025-09-05T06:59:26.201446Z","caller":"traceutil/trace.go:171","msg":"trace[1971811333] range","detail":"{range_begin:/registry/health; range_end:; }","duration":"2.000953685s","start":"2025-09-05T06:59:24.200475Z","end":"2025-09-05T06:59:26.201429Z","steps":["trace[1971811333] 'agreement among raft nodes before linearized reading'  (duration: 2.00084869s)"],"step_count":1}
{"level":"warn","ts":"2025-09-05T06:59:26.201504Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:24.200458Z","time spent":"2.001035281s","remote":"127.0.0.1:49834","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":0,"request content":"key:\"/registry/health\" "}
2025/09/05 06:59:26 WARNING: [core] [Server #7] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2025-09-05T06:59:26.558054Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238537935495157233,"retry-timeout":"500ms"}
{"level":"warn","ts":"2025-09-05T06:59:27.059168Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238537935495157233,"retry-timeout":"500ms"}
{"level":"warn","ts":"2025-09-05T06:59:27.437386Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.380062269s","expected-duration":"1s"}
{"level":"warn","ts":"2025-09-05T06:59:28.209543Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.00063907s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"","error":"context deadline exceeded"}
{"level":"info","ts":"2025-09-05T06:59:28.209650Z","caller":"traceutil/trace.go:171","msg":"trace[853168249] range","detail":"{range_begin:/registry/health; range_end:; }","duration":"2.000798663s","start":"2025-09-05T06:59:26.208827Z","end":"2025-09-05T06:59:28.209626Z","steps":["trace[853168249] 'agreement among raft nodes before linearized reading'  (duration: 2.000668369s)"],"step_count":1}
{"level":"warn","ts":"2025-09-05T06:59:28.209713Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:26.208810Z","time spent":"2.000889959s","remote":"127.0.0.1:49848","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":0,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-09-05T06:59:29.613037Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.175384356s","expected-duration":"100ms","prefix":"","request":"header:<ID:3238537935495157232 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:9487 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2025-09-05T06:59:29.613165Z","caller":"traceutil/trace.go:171","msg":"trace[634652222] transaction","detail":"{read_only:false; response_revision:9492; number_of_response:1; }","duration":"4.122861473s","start":"2025-09-05T06:59:25.490279Z","end":"2025-09-05T06:59:29.613140Z","steps":["trace[634652222] 'process raft request'  (duration: 1.947255227s)","trace[634652222] 'compare'  (duration: 2.175237962s)"],"step_count":2}
{"level":"warn","ts":"2025-09-05T06:59:29.613239Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:25.490252Z","time spent":"4.122950869s","remote":"127.0.0.1:50270","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:9487 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2025-09-05T06:59:30.209943Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.999528319s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"","error":"context canceled"}
{"level":"info","ts":"2025-09-05T06:59:30.210071Z","caller":"traceutil/trace.go:171","msg":"trace[504471691] range","detail":"{range_begin:/registry/health; range_end:; }","duration":"1.999707911s","start":"2025-09-05T06:59:28.210343Z","end":"2025-09-05T06:59:30.210051Z","steps":["trace[504471691] 'agreement among raft nodes before linearized reading'  (duration: 1.999557118s)"],"step_count":1}
{"level":"warn","ts":"2025-09-05T06:59:30.210130Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:28.210323Z","time spent":"1.999796107s","remote":"127.0.0.1:49834","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":0,"request content":"key:\"/registry/health\" "}
2025/09/05 06:59:30 WARNING: [core] [Server #7] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2025-09-05T06:59:30.377910Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"2.940322383s","expected-duration":"1s"}
{"level":"warn","ts":"2025-09-05T06:59:31.506960Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.128696767s","expected-duration":"100ms","prefix":"","request":"header:<ID:3238537935495157234 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/kube-system/kube-apiserver-minikube\" mod_revision:9489 > success:<request_put:<key:\"/registry/pods/kube-system/kube-apiserver-minikube\" value_size:7411 >> failure:<request_range:<key:\"/registry/pods/kube-system/kube-apiserver-minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2025-09-05T06:59:31.507116Z","caller":"traceutil/trace.go:171","msg":"trace[735360943] linearizableReadLoop","detail":"{readStateIndex:11796; appliedIndex:11794; }","duration":"5.449556598s","start":"2025-09-05T06:59:26.057542Z","end":"2025-09-05T06:59:31.507099Z","steps":["trace[735360943] 'read index received'  (duration: 1.380062569s)","trace[735360943] 'applied index is now lower than readState.Index'  (duration: 4.069493029s)"],"step_count":2}
{"level":"warn","ts":"2025-09-05T06:59:31.507223Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"7.919528114s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-05T06:59:31.507262Z","caller":"traceutil/trace.go:171","msg":"trace[40715850] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:9493; }","duration":"7.919569512s","start":"2025-09-05T06:59:23.587682Z","end":"2025-09-05T06:59:31.507252Z","steps":["trace[40715850] 'agreement among raft nodes before linearized reading'  (duration: 7.919511515s)"],"step_count":1}
{"level":"info","ts":"2025-09-05T06:59:31.507290Z","caller":"traceutil/trace.go:171","msg":"trace[147599445] transaction","detail":"{read_only:false; response_revision:9493; number_of_response:1; }","duration":"5.439542941s","start":"2025-09-05T06:59:26.067676Z","end":"2025-09-05T06:59:31.507219Z","steps":["trace[147599445] 'process raft request'  (duration: 4.310503288s)","trace[147599445] 'compare'  (duration: 1.128388381s)"],"step_count":2}
{"level":"warn","ts":"2025-09-05T06:59:31.507369Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"5.440298707s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:420"}
{"level":"warn","ts":"2025-09-05T06:59:31.507396Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:26.067658Z","time spent":"5.439686434s","remote":"127.0.0.1:50194","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":7469,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/pods/kube-system/kube-apiserver-minikube\" mod_revision:9489 > success:<request_put:<key:\"/registry/pods/kube-system/kube-apiserver-minikube\" value_size:7411 >> failure:<request_range:<key:\"/registry/pods/kube-system/kube-apiserver-minikube\" > >"}
{"level":"warn","ts":"2025-09-05T06:59:31.507464Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.289958847s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-05T06:59:31.507488Z","caller":"traceutil/trace.go:171","msg":"trace[234304705] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:9493; }","duration":"1.289983146s","start":"2025-09-05T06:59:30.217497Z","end":"2025-09-05T06:59:31.507481Z","steps":["trace[234304705] 'agreement among raft nodes before linearized reading'  (duration: 1.289943048s)"],"step_count":1}
{"level":"warn","ts":"2025-09-05T06:59:31.507515Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:30.217440Z","time spent":"1.290067743s","remote":"127.0.0.1:49848","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-09-05T06:59:31.507519Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"3.244702944s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingadmissionpolicies/\" range_end:\"/registry/validatingadmissionpolicies0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-05T06:59:31.507554Z","caller":"traceutil/trace.go:171","msg":"trace[1414291915] range","detail":"{range_begin:/registry/validatingadmissionpolicies/; range_end:/registry/validatingadmissionpolicies0; response_count:0; response_revision:9493; }","duration":"3.24477784s","start":"2025-09-05T06:59:28.262763Z","end":"2025-09-05T06:59:31.507541Z","steps":["trace[1414291915] 'agreement among raft nodes before linearized reading'  (duration: 3.244724743s)"],"step_count":1}
{"level":"warn","ts":"2025-09-05T06:59:31.507583Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:28.262739Z","time spent":"3.244835738s","remote":"127.0.0.1:50556","response type":"/etcdserverpb.KV/Range","request count":0,"request size":82,"response count":0,"response size":28,"request content":"key:\"/registry/validatingadmissionpolicies/\" range_end:\"/registry/validatingadmissionpolicies0\" count_only:true "}
{"level":"warn","ts":"2025-09-05T06:59:31.507726Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"3.892107761s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csidrivers/\" range_end:\"/registry/csidrivers0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-05T06:59:31.507757Z","caller":"traceutil/trace.go:171","msg":"trace[1294001825] range","detail":"{range_begin:/registry/csidrivers/; range_end:/registry/csidrivers0; response_count:0; response_revision:9493; }","duration":"3.892142259s","start":"2025-09-05T06:59:27.615606Z","end":"2025-09-05T06:59:31.507749Z","steps":["trace[1294001825] 'agreement among raft nodes before linearized reading'  (duration: 3.892089761s)"],"step_count":1}
{"level":"warn","ts":"2025-09-05T06:59:31.507771Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.9138998s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles/\" range_end:\"/registry/roles0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"warn","ts":"2025-09-05T06:59:31.507785Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:27.615583Z","time spent":"3.892195257s","remote":"127.0.0.1:50442","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":0,"response size":28,"request content":"key:\"/registry/csidrivers/\" range_end:\"/registry/csidrivers0\" count_only:true "}
{"level":"info","ts":"2025-09-05T06:59:31.507805Z","caller":"traceutil/trace.go:171","msg":"trace[605103133] range","detail":"{range_begin:/registry/roles/; range_end:/registry/roles0; response_count:0; response_revision:9493; }","duration":"1.913991696s","start":"2025-09-05T06:59:29.593803Z","end":"2025-09-05T06:59:31.507795Z","steps":["trace[605103133] 'agreement among raft nodes before linearized reading'  (duration: 1.913929599s)"],"step_count":1}
{"level":"warn","ts":"2025-09-05T06:59:31.507832Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:29.593782Z","time spent":"1.914042594s","remote":"127.0.0.1:50370","response type":"/etcdserverpb.KV/Range","request count":0,"request size":38,"response count":12,"response size":30,"request content":"key:\"/registry/roles/\" range_end:\"/registry/roles0\" count_only:true "}
{"level":"info","ts":"2025-09-05T06:59:31.507417Z","caller":"traceutil/trace.go:171","msg":"trace[1434052712] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:9493; }","duration":"5.440397002s","start":"2025-09-05T06:59:26.067008Z","end":"2025-09-05T06:59:31.507405Z","steps":["trace[1434052712] 'agreement among raft nodes before linearized reading'  (duration: 5.440302206s)"],"step_count":1}
{"level":"warn","ts":"2025-09-05T06:59:31.507923Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:26.066953Z","time spent":"5.440959278s","remote":"127.0.0.1:50182","response type":"/etcdserverpb.KV/Range","request count":0,"request size":51,"response count":1,"response size":443,"request content":"key:\"/registry/services/endpoints/default/kubernetes\" limit:1 "}
{"level":"warn","ts":"2025-09-05T06:59:31.508035Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"4.194542207s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/\" range_end:\"/registry/clusterroles0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-09-05T06:59:31.508092Z","caller":"traceutil/trace.go:171","msg":"trace[1027990460] range","detail":"{range_begin:/registry/clusterroles/; range_end:/registry/clusterroles0; response_count:0; response_revision:9493; }","duration":"4.194649404s","start":"2025-09-05T06:59:27.313430Z","end":"2025-09-05T06:59:31.508080Z","steps":["trace[1027990460] 'agreement among raft nodes before linearized reading'  (duration: 4.19450301s)"],"step_count":1}
{"level":"warn","ts":"2025-09-05T06:59:31.508127Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-05T06:59:27.313406Z","time spent":"4.1947108s","remote":"127.0.0.1:50396","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":68,"response size":30,"request content":"key:\"/registry/clusterroles/\" range_end:\"/registry/clusterroles0\" count_only:true "}


==> kernel <==
 07:00:55 up 10:49,  0 users,  load average: 2.81, 2.86, 3.22
Linux minikube 6.8.0-1026-azure #31~22.04.1-Ubuntu SMP Thu Mar 20 04:12:50 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [973f949e777e] <==
I0905 03:56:39.341274       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0905 03:56:39.439492       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0905 03:56:39.453962       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0905 03:56:42.942055       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0905 03:56:43.450468       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 03:56:43.458103       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 03:56:44.220455       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
E0905 04:01:50.183000       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0905 04:01:50.183917       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0905 04:02:05.707241       1 controller.go:163] "Unhandled Error" err="unable to sync kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service" logger="UnhandledError"
I0905 04:06:36.232133       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
E0905 04:13:17.804326       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0905 04:13:17.804431       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 20.299µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0905 04:13:17.805825       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0905 04:13:17.807009       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0905 04:13:17.808289       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="3.951928ms" method="PUT" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0905 04:13:20.429088       1 controller.go:163] "Unhandled Error" err="unable to sync kubernetes service: etcdserver: request timed out" logger="UnhandledError"
E0905 04:13:20.429931       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0905 04:13:51.044456       1 controller.go:163] "Unhandled Error" err="unable to sync kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service" logger="UnhandledError"
I0905 04:16:36.232509       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 04:26:36.232661       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 04:36:36.233074       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 04:46:36.233896       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 04:56:36.234558       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 05:06:36.234944       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 05:16:36.235293       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 05:26:36.236448       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
E0905 05:29:24.812699       1 conn.go:339] Error on socket receive: read tcp 192.168.58.2:8443->192.168.58.1:36098: use of closed network connection
E0905 05:32:02.901240       1 conn.go:339] Error on socket receive: read tcp 192.168.58.2:8443->192.168.58.1:47098: use of closed network connection
I0905 05:36:36.236133       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 05:46:36.236902       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
E0905 05:53:38.312051       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0905 05:53:38.361027       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0905 05:53:40.991027       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"context canceled\"}: context canceled" logger="UnhandledError"
E0905 05:53:40.992664       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0905 05:53:40.993849       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0905 05:53:40.996087       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0905 05:53:40.997328       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="6.314523ms" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
I0905 05:56:36.237049       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
E0905 06:03:08.884857       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0905 06:03:08.927070       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
E0905 06:03:10.072170       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0905 06:03:10.072190       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 9.9µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0905 06:03:10.073343       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0905 06:03:10.074519       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0905 06:03:10.075849       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="3.756334ms" method="POST" path="/api/v1/namespaces/kube-system/events" result=null
I0905 06:06:36.238615       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 06:16:36.240259       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 06:26:36.240284       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 06:36:36.240543       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 06:46:36.240744       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0905 06:56:36.241430       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
E0905 06:58:02.156527       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0905 06:58:02.156701       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 98.496µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0905 06:58:02.158017       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0905 06:58:02.161170       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0905 06:58:02.163043       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="6.30122ms" method="PUT" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0905 06:58:02.164573       1 wrap.go:53] "Timeout or abort while handling" logger="UnhandledError" method="POST" URI="/api/v1/namespaces/kube-system/events" auditID="b6795e46-60db-4a33-a549-c1448f759670"
E0905 06:58:02.164631       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="3.1µs" method="POST" path="/api/v1/namespaces/kube-system/events" result=null
E0905 06:58:02.869773       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 705.216809ms, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"


==> kube-controller-manager [86ce0bfab5c4] <==
I0905 03:56:42.690565       1 expand_controller.go:329] "Starting expand controller" logger="persistentvolume-expander-controller"
I0905 03:56:42.690588       1 shared_informer.go:350] "Waiting for caches to sync" controller="expand"
I0905 03:56:42.840163       1 controllermanager.go:778] "Started controller" controller="persistentvolume-protection-controller"
I0905 03:56:42.840199       1 pv_protection_controller.go:81] "Starting PV protection controller" logger="persistentvolume-protection-controller"
I0905 03:56:42.840221       1 shared_informer.go:350] "Waiting for caches to sync" controller="PV protection"
I0905 03:56:42.852424       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0905 03:56:42.876805       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0905 03:56:42.887631       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0905 03:56:42.891058       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0905 03:56:42.892283       1 shared_informer.go:357] "Caches are synced" controller="job"
I0905 03:56:42.892317       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0905 03:56:42.909556       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0905 03:56:42.909597       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0905 03:56:42.910473       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0905 03:56:42.912741       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0905 03:56:42.927023       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0905 03:56:42.939608       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0905 03:56:42.939666       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0905 03:56:42.940620       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0905 03:56:42.941746       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0905 03:56:42.941779       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0905 03:56:42.941815       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0905 03:56:42.943038       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0905 03:56:42.943087       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0905 03:56:42.944175       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0905 03:56:42.946560       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0905 03:56:42.958577       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0905 03:56:42.964838       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0905 03:56:42.989089       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0905 03:56:42.991362       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0905 03:56:43.010570       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0905 03:56:43.016100       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0905 03:56:43.050260       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0905 03:56:43.077399       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0905 03:56:43.102949       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0905 03:56:43.117758       1 shared_informer.go:357] "Caches are synced" controller="node"
I0905 03:56:43.117927       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0905 03:56:43.118108       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0905 03:56:43.118127       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0905 03:56:43.118135       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0905 03:56:43.119924       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0905 03:56:43.132503       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0905 03:56:43.132605       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0905 03:56:43.140398       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0905 03:56:43.142940       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0905 03:56:43.146173       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0905 03:56:43.146478       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0905 03:56:43.146619       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0905 03:56:43.146696       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0905 03:56:43.153414       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0905 03:56:43.190372       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0905 03:56:43.192736       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0905 03:56:43.213364       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0905 03:56:43.224917       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0905 03:56:43.295428       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0905 03:56:43.677503       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0905 03:56:43.739064       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0905 03:56:43.739094       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0905 03:56:43.739111       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0905 05:56:38.324301       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-kw88k" approvedExpiration="1h0m0s"


==> kube-proxy [46f125738545] <==
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0905 06:56:38.620494       1 proxier.go:820] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0905 06:57:08.760970       1 proxier.go:1553] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0905 06:57:08.761064       1 proxier.go:820] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0905 06:57:38.877077       1 proxier.go:1553] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0905 06:57:38.877131       1 proxier.go:820] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0905 06:58:09.034207       1 proxier.go:1553] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0905 06:58:09.034287       1 proxier.go:820] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0905 06:58:39.164667       1 proxier.go:1553] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0905 06:58:39.164731       1 proxier.go:820] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0905 06:59:09.292846       1 proxier.go:1553] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0905 06:59:09.292901       1 proxier.go:820] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0905 06:59:39.419518       1 proxier.go:1553] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0905 06:59:39.419572       1 proxier.go:820] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0905 07:00:09.565110       1 proxier.go:1553] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0905 07:00:09.565231       1 proxier.go:820] "Sync failed" ipFamily="IPv6" retryingTime="30s"
E0905 07:00:39.628432       1 proxier.go:1553] "Failed to execute iptables-restore" err=<
	exit status 2: Warning: Extension MARK revision 0 not supported, missing kernel module?
	ip6tables-restore v1.8.9 (nf_tables): unknown option "--xor-mark"
	Error occurred at line: 17
	Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.
 > ipFamily="IPv6"
I0905 07:00:39.628463       1 proxier.go:820] "Sync failed" ipFamily="IPv6" retryingTime="30s"


==> kube-scheduler [89d34111aadf] <==
I0905 03:56:33.964409       1 serving.go:386] Generated self-signed cert in-memory
W0905 03:56:36.224487       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0905 03:56:36.224550       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0905 03:56:36.224567       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0905 03:56:36.224579       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0905 03:56:36.328414       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0905 03:56:36.328454       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0905 03:56:36.332513       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0905 03:56:36.332560       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0905 03:56:36.333919       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
I0905 03:56:36.334036       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0905 03:56:36.334179       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
E0905 03:56:36.337599       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0905 03:56:36.337747       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0905 03:56:36.404527       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0905 03:56:36.404637       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0905 03:56:36.404671       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0905 03:56:36.404768       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0905 03:56:36.404878       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0905 03:56:36.405004       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0905 03:56:36.405137       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0905 03:56:36.405217       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0905 03:56:36.405282       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0905 03:56:36.405323       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0905 03:56:36.405376       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0905 03:56:36.405376       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0905 03:56:36.407248       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0905 03:56:37.280274       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0905 03:56:37.309806       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0905 03:56:37.366221       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0905 03:56:37.385669       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0905 03:56:37.463175       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0905 03:56:37.588336       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0905 03:56:37.621129       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0905 03:56:37.630524       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0905 03:56:37.644116       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
I0905 03:56:40.833425       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Sep 05 06:54:34 minikube kubelet[2868]: I0905 06:54:34.419516    2868 scope.go:117] "RemoveContainer" containerID="8ff3d98ea70ac95f709343ff7313c31c47b9a2c841b2d81fa5dc9cea44a59d81"
Sep 05 06:54:34 minikube kubelet[2868]: E0905 06:54:34.419773    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:54:47 minikube kubelet[2868]: I0905 06:54:47.420394    2868 scope.go:117] "RemoveContainer" containerID="8ff3d98ea70ac95f709343ff7313c31c47b9a2c841b2d81fa5dc9cea44a59d81"
Sep 05 06:55:05 minikube kubelet[2868]: I0905 06:55:05.388853    2868 scope.go:117] "RemoveContainer" containerID="8ff3d98ea70ac95f709343ff7313c31c47b9a2c841b2d81fa5dc9cea44a59d81"
Sep 05 06:55:05 minikube kubelet[2868]: I0905 06:55:05.389520    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:55:05 minikube kubelet[2868]: E0905 06:55:05.389805    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:55:19 minikube kubelet[2868]: I0905 06:55:19.419553    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:55:19 minikube kubelet[2868]: E0905 06:55:19.419801    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:55:31 minikube kubelet[2868]: I0905 06:55:31.420396    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:55:31 minikube kubelet[2868]: E0905 06:55:31.420714    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:55:46 minikube kubelet[2868]: I0905 06:55:46.420425    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:55:46 minikube kubelet[2868]: E0905 06:55:46.420653    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:56:01 minikube kubelet[2868]: I0905 06:56:01.419958    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:56:01 minikube kubelet[2868]: E0905 06:56:01.420394    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:56:13 minikube kubelet[2868]: I0905 06:56:13.420424    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:56:13 minikube kubelet[2868]: E0905 06:56:13.420731    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:56:25 minikube kubelet[2868]: I0905 06:56:25.420504    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:56:25 minikube kubelet[2868]: E0905 06:56:25.420777    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:56:40 minikube kubelet[2868]: I0905 06:56:40.420061    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:56:40 minikube kubelet[2868]: E0905 06:56:40.420364    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:56:53 minikube kubelet[2868]: I0905 06:56:53.420458    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:56:53 minikube kubelet[2868]: E0905 06:56:53.420792    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:57:08 minikube kubelet[2868]: I0905 06:57:08.420174    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:57:08 minikube kubelet[2868]: E0905 06:57:08.420595    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:57:22 minikube kubelet[2868]: I0905 06:57:22.420516    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:57:22 minikube kubelet[2868]: E0905 06:57:22.420835    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:57:35 minikube kubelet[2868]: I0905 06:57:35.419770    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:57:35 minikube kubelet[2868]: E0905 06:57:35.420145    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:57:47 minikube kubelet[2868]: I0905 06:57:47.420962    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:57:47 minikube kubelet[2868]: E0905 06:57:47.421342    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:57:59 minikube kubelet[2868]: I0905 06:57:59.420689    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:57:59 minikube kubelet[2868]: E0905 06:57:59.421054    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:58:12 minikube kubelet[2868]: I0905 06:58:12.420446    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:58:12 minikube kubelet[2868]: E0905 06:58:12.420795    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:58:23 minikube kubelet[2868]: I0905 06:58:23.420070    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:58:23 minikube kubelet[2868]: E0905 06:58:23.420369    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:58:36 minikube kubelet[2868]: I0905 06:58:36.420591    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:58:36 minikube kubelet[2868]: E0905 06:58:36.420898    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:58:50 minikube kubelet[2868]: I0905 06:58:50.420181    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:58:50 minikube kubelet[2868]: E0905 06:58:50.420466    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:59:02 minikube kubelet[2868]: I0905 06:59:02.420176    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:59:02 minikube kubelet[2868]: E0905 06:59:02.420512    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:59:16 minikube kubelet[2868]: I0905 06:59:16.420445    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:59:16 minikube kubelet[2868]: E0905 06:59:16.420729    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:59:31 minikube kubelet[2868]: I0905 06:59:31.420593    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:59:31 minikube kubelet[2868]: E0905 06:59:31.420908    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:59:31 minikube kubelet[2868]: I0905 06:59:31.812501    2868 scope.go:117] "RemoveContainer" containerID="02614962122dca9a187f0d42275232c2853c3dae9862ed901b5c6cc097270305"
Sep 05 06:59:31 minikube kubelet[2868]: I0905 06:59:31.813359    2868 scope.go:117] "RemoveContainer" containerID="fe3b3c9de0047c8de319be8d9bcc3a3a067e49d3a1b7ace744bc2fc1a33fa8d3"
Sep 05 06:59:44 minikube kubelet[2868]: I0905 06:59:44.420605    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:59:44 minikube kubelet[2868]: E0905 06:59:44.420930    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 06:59:56 minikube kubelet[2868]: I0905 06:59:56.419645    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 06:59:56 minikube kubelet[2868]: E0905 06:59:56.419965    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 07:00:10 minikube kubelet[2868]: I0905 07:00:10.420456    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 07:00:27 minikube kubelet[2868]: I0905 07:00:27.653300    2868 scope.go:117] "RemoveContainer" containerID="b7d39293539d6f5f54fd5c52667294e71e1420caeafaf805a66091ba094fb88a"
Sep 05 07:00:27 minikube kubelet[2868]: I0905 07:00:27.653677    2868 scope.go:117] "RemoveContainer" containerID="0862aaf3f2710370a1d1c3ef4deba89df223f6a4d1c67c834003ba3ba237095e"
Sep 05 07:00:27 minikube kubelet[2868]: E0905 07:00:27.653817    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 07:00:39 minikube kubelet[2868]: I0905 07:00:39.419920    2868 scope.go:117] "RemoveContainer" containerID="0862aaf3f2710370a1d1c3ef4deba89df223f6a4d1c67c834003ba3ba237095e"
Sep 05 07:00:39 minikube kubelet[2868]: E0905 07:00:39.420088    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"
Sep 05 07:00:54 minikube kubelet[2868]: I0905 07:00:54.420412    2868 scope.go:117] "RemoveContainer" containerID="0862aaf3f2710370a1d1c3ef4deba89df223f6a4d1c67c834003ba3ba237095e"
Sep 05 07:00:54 minikube kubelet[2868]: E0905 07:00:54.420562    2868 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mnist-gpu\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mnist-gpu pod=mnist-gpu-cf678c46f-hbcmv_mnist-gpu-demo(1d855587-ecaf-43c9-ba32-da272b0d5a04)\"" pod="mnist-gpu-demo/mnist-gpu-cf678c46f-hbcmv" podUID="1d855587-ecaf-43c9-ba32-da272b0d5a04"


==> storage-provisioner [580594a4c8f2] <==
W0905 06:59:57.777874       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 06:59:57.789107       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 06:59:59.792698       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 06:59:59.798598       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:01.806207       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:01.816503       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:03.821273       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:03.828479       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:05.832526       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:05.842057       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:07.846550       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:07.853284       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:09.856863       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:09.863231       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:11.870966       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:11.881245       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:13.905454       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:13.914925       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:15.922935       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:15.932622       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:17.935600       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:17.942497       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:19.947923       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:19.955937       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:21.961180       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:21.970784       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:23.975091       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:23.984810       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:25.987317       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:25.991402       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:27.993675       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:27.997633       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:29.999610       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:30.003626       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:32.005246       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:32.012247       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:34.014969       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:34.020654       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:36.023572       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:36.027508       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:38.029829       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:38.033720       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:40.036312       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:40.043090       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:42.045068       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:42.049435       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:44.052718       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:44.063695       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:46.066379       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:46.070446       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:48.072988       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:48.076849       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:50.079251       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:50.086798       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:52.089320       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:52.093444       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:54.096409       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:54.100298       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:56.162402       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0905 07:00:56.231671       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [fe3b3c9de004] <==
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe

goroutine 4605 [runnable]:
k8s.io/client-go/tools/record.(*recorderImpl).generateEvent.func1(0xc00009c680, 0xc00039ea00)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/tools/record/event.go:341
created by k8s.io/client-go/tools/record.(*recorderImpl).generateEvent
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/tools/record/event.go:341 +0x3b7

goroutine 4410 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0xc0000d49e0, 0x2)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc0000d49d0)
	/usr/local/go/src/sync/cond.go:56 +0x99
golang.org/x/net/http2.(*pipe).Read(0xc0000d49c8, 0xc0000ba601, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x97
golang.org/x/net/http2.transportResponseBody.Read(0xc0000d49a0, 0xc0000ba601, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0xaf
encoding/json.(*Decoder).refill(0xc0000d4c60, 0xa, 0x9)
	/usr/local/go/src/encoding/json/stream.go:165 +0xeb
encoding/json.(*Decoder).readValue(0xc0000d4c60, 0x0, 0x0, 0x152aee0)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1ff
encoding/json.(*Decoder).Decode(0xc0000d4c60, 0x154a160, 0xc00053a168, 0x203000, 0x203000)
	/usr/local/go/src/encoding/json/stream.go:63 +0x7c
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0xc000379c50, 0xc0000e8000, 0x400, 0x400, 0x40, 0x38, 0x15b0440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x1a8
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc000467900, 0x0, 0x18bc168, 0xc00059e200, 0x0, 0x0, 0x461dc0, 0xc00024ea20, 0xc000585e50)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc00058ee20, 0xc000585ef0, 0x8, 0x18baa20, 0xc0001b7400, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc00025d680)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe

goroutine 4431 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0xc0001a6720, 0x2)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc0001a6710)
	/usr/local/go/src/sync/cond.go:56 +0x99
golang.org/x/net/http2.(*pipe).Read(0xc0001a6708, 0xc0000bac01, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x97
golang.org/x/net/http2.transportResponseBody.Read(0xc0001a66e0, 0xc0000bac01, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0xaf
encoding/json.(*Decoder).refill(0xc0001a69a0, 0xa, 0x9)
	/usr/local/go/src/encoding/json/stream.go:165 +0xeb
encoding/json.(*Decoder).readValue(0xc0001a69a0, 0x0, 0x0, 0x152aee0)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1ff
encoding/json.(*Decoder).Decode(0xc0001a69a0, 0x154a160, 0xc0004b6798, 0x203000, 0x203000)
	/usr/local/go/src/encoding/json/stream.go:63 +0x7c
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0xc000246480, 0xc0001a3000, 0x400, 0x400, 0x40, 0x38, 0x15b0440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x1a8
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc00051a780, 0x0, 0x18bc168, 0xc000189480, 0x0, 0x0, 0x461dc0, 0xc0006eb080, 0xc00010fe50)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc000518fe0, 0xc00010fef0, 0x8, 0x18bbba0, 0xc0002c5980, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc000188780)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe

